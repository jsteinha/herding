CITE "Online Learning and Online Convex Optimization". Shai Shalev-Shwartz.
http://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf
-general survey (good to follow references)

Fenchel: [40, 39, 24]
EG: [26, 6, 28]
Local norms: [2, 34]
Hinge loss <-> Perceptron: [37, 3, 33, 18]
Winnow algorithm: max(0, 1-y(2<w,x>-1))  *** this is most closely related to our setings ***: [29, 5]

LOOK AT LATER Distributional assumptions:
"Online Learning of Noisy Data" Nicolo Cesa-Bianchi, Shai Shalev-Shwartz and Ohad Shamir, 2011
"Online Learning of Noisy Data with Kernels", 2010
"Learning from Noisy Data under Distributional Assumptions", 2010
  -these are probably not relevant, they look at perturbing (xt, yt) by some noise to make the problem harder

Exponentiated gradient:
DATA/CITE Stochastic Methods for l1-regularized Loss Minimization", 2011
  -look at their experimental setup, has good datasets
  -they also cite Langford 2009 which presumably also has good datasets
CITE Learning Sparse Low-Threshold Linear Classifiers, 2012
-more delicate analysis of unnormalized-EG for hinge loss to get O(k*theta) instead of O(k^2)
CITE J. Kivinen and M. Warmuth, “Exponentiated gradient versus gradient descent
for linear predictors,” 1997
-original EG algorithm
J. Kivinen and M. K. Warmuth, “Additive versus exponentiated gradient
updates for linear prediction,” STOC 1995;
-conference version of the above
MAYBE K. Azoury and M. Warmuth, “Relative loss bounds for on-line density estima-
tion with the exponential family of distributions,” 1999(?)
-something like EG, with log loss
J. Kivinen and M. Warmuth, “Relative loss bounds for multidimensional regres-
sion problems,” 2001
-seems like an early version of mirror descent, not that relevant


  Winnow:
  MAYBE CITE P. Auer and M. Warmuth, “Tracking the best disjunction”
  -tracks a time-varying disjunction, basically time-varying winnow, could be good to try to adapt our analysis to this case
  -probably not worth it though
  CITE N. Littlestone, “Learning quickly when irrelevant attributes abound: A new
    linear-threshold algorithm,” 1988
  -paper originally introducing the Winndow algorithm
  N. Littlestone, “Mistake bounds and logarithmic linear-threshold learning algo-
    rithms,” Ph.D. Thesis, University of California at Santa Cruz, 1990.
  -couldn't get a copy easily



Perceptron:
MAYBE F. Rosenblatt, “The perceptron: A probabilistic model for information storage
and organization in the brain,” 1958
-original perceptron paper
Y. Freund and R. E. Schapire, “Large margin classification using the perceptron
algorithm,” 1999
-not clear


Fenchel:
CITE S. Shalev-Shwartz, “Online learning: Theory, algorithms, and applications,”
 Ph.D. Thesis, The Hebrew University, 2007.
-casts boosting as a game between a weak learner and booster
-REALLY pretty use of duality in Chapter 3
-claims to get efficient algorithms for structured prediction (see Eqn. 5.24)
  -see also "Online passive aggressive algorithms", Crammer et al.


S. Shalev-Shwartz and Y. Singer, “A primal-dual perspective of online learning
algorithms,” Machine Learning Journal, vol. 69, no. 2, pp. 115–142, 2007.
-looks like it's subsumed by the Thesis

Adaptive:
READ, CITE "Adaptive bound optimization for online convex optimization"
CITE Dimension-free exponentiated gradient
MAYBE CITE Second-Order Non-Stationary Online Learning for Regression
-Table 1 could provide useful inspiration
CITE Less Regret via Online Conditioning
  -adaptive step size for additive updates (or potentially decomposable updates more generally)
CITE A Generalized Online Mirror Descent with Applications to Classification and Regression
  -Lemma 1 in their paper is the main proof technique we use
CITE Adaptive regularization of weight vectors
  -AROW paper

A second-order Perceptron algorithm
Efficient online and batch learning using forward backward splitting
Composite objective mirror descent
Dual averaging methods for regularized stochastic learning and online optimization

Convex games in banach spaces

"Adaptive Bound Optimization for Online Convex Optimization"
"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"



Boosting / Frank-Wolfe (LATER):
CITE "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", 2011
-super-relevant from the perspective of the Frank-Wolfe stuff, they basically use the 
  fixed-step-size version to solve an SVM
READ Stochastic Methods for l1-regularized Loss Minimization", 2011
  -VERY relevant to Frank-Wolfe
  -may also have good experiments / references
  -look at the references listed in the intro for other good related work
Sparse online learning via truncated gradient, 2009
  -could be worth citing for Frank-Wolfe (does online version of truncated gradients)
“Online learning: Theory, algorithms, and applications”
Logistic regression, AdaBoost and Bregman distances
Additive logistic regression: a statistical view of boosting.
Functional gradient techniques for combining hypotheses
Totally corrective boosting algorithms that maximize the margin
A decision-theoretic generalization of on-line learning and an application to boosting
"Mind the duality gap: Logarithmic regret algorithms for online optimization", 2008



Misc. recent:
Improved boosting algorithms using confidence-rated predictions, 1999
  -allows hypotheses to assign confidences to their predictions
"Smoothness, low-noise and fast rates", 2010
  -focuses on ERM
"On the universality of online mirror descent", 2011
  -matching upper and lower bounds up to O(log^2(n)) factor
"Learning Optimally Sparse Support Vector Machines", 2013
  -performs a post-processing step on SVMS to get out a sparse solution
"Vanishing Component Analysis", 2013
  -some algebraic geometry stuff
"Learning the Experts for Online Sequence Prediction", 2012
  -first learns the experts given side info, then does MWU
CITE "Regularization Techniques for Learning with Matrices" Sham Kakade, Shai Shalev-Shwartz, Ambuj Tewari. 2012. 
  -good suggestions for matrix regularizers, should consider whether our method generalizes to Lp norm for small p
"Learning Kernel Based Halfspaces with the 0-1 Loss" Shai Shalev-Shwartz, Karthik Sridharan and Ohad Shamir, 2011
  -not relevant
MAYBE READ "Trading Accuracy for Sparsity in Optimization Problems with Sparsity Constraints", 2010
A. Rakhlin, Lecture Notes on Online Learning. draft, 2009.
  -possibly should look at online shortest path problem?
Mirror descent and nonlinear projected subgradient methods for convex optimization, 2003
  -general mirror descent exposition
  -NOTE: first paper seems to be by Nemirovski and Yudin
A. Rakhlin, K. Sridharan, and A. Tewari, “Online learning: Random averages, combinatorial parameters, and learnability,” 2010.
  -not too relevant here, might be relevant to Paul
J. Abernethy, E. Hazan, and A. Rakhlin, “Competing in the dark: An efficient algorithm for bandit linear optimization,” 2008
  -self-concordance
  -could be useful in the conclusion
CITE Online Learning: Stochastic, Constrained, and Smoothed Adversaries
CITE "Online Learning with Predictable Sequences", Alexander Rakhlin and Karthik Sridharan.
  -shows how to achieve better bounds if we know roughly the behavior of the sequence
CITE Extracting Certainty from Uncertainty: Regret Bounded by Variation in Costs
  -SUPER important; they have an update that is *very* similar to ours
  -also get nicer bounds, e.g. in terms of variance rather than L2 norm
CITE Better Algorithms for Benign Bandits
  -bandit setting of preceding paper
Online Optimization with Gradual Variations
  -path length bounds
CITE Improved second-order bounds for prediction with expert advice
  -SUPER important; they explicitly compare MWU to MD (aka Weighted Majority) and point out the relationship between the bounds
CITE Adaptive Online Gradient Descent
  -adaptive in face of unknown degree of convexity






Interesting but unrelated:
Competitive on-line statistics
  also: Prequential probability: principles and properties, 1997
Ultraconservative online algorithms for multiclass problems, 2003
-introduces MIRA
"Efficient Learning with Partially Observed Attributes. Nicolo Cesa-Bianchi, Shai Shalev-Shwartz and Ohad Shamir, 2011
"Pegasos: Primal Estimated sub-GrAdient SOlver for SVM" , 2011
--> Shai seems to work in general on "budgeted optimization"
"ShareBoost: Efficient multiclass learning with feature sharing", 2012
"Some Impossibility Results for Budgeted Learning", 2010
"Announcement: The Generalization Ability of Online Algorithms for Dependent Data", COLT 2012
"Online Learning of Complex Prediction Problems Using Simultaneous Projections", 2008
"Online Learning: Beyond Regret"
  also: "Unified Algorithms for Online Learning and Competitive Analysis"
  also maybe: "A Tale of Two Metrics: Simultaneous Bounds on Competitiveness and Regret"
Look up: Martingale empirical process theory
"Opportunistic Strategies for Generalized No-Regret Problems", Andrey Bernstein, Shie Mannor and Nahum Shimkin.
  -and notion of "approachability" more generally
"Regret Minimization for Branching Experts"
"Regret Bounds for Sleeping Experts and Bandits"
Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression


Clarifying results:
S. Ben-David, D. Pal, and S. Shalev-Shwartz, “Agnostic online learning,” 2009
"Stochastic Convex Optimization", Shai Shalev-Shwartz, Ohad Shamir, Karthik Sridharan and Nati Srebro COLT, 2009.
Convex games in banach spaces

