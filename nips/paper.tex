\documentclass{article} % For LaTeX2e
\usepackage{import, subfiles}
\usepackage{tabularx}
\usepackage{nips13submit_e,times}
\input{latex-defs.tex}

\title{A Greedy Framework for First-Order Optimization}
\author{
Jacob Steinhardt\thanks{Both authors contributed equally to this work.} \\
Department of Computer Science\\
Stanford University\\
Stanford, CA 94305 \\
\texttt{jsteinhardt@cs.stanford.edu} \\
\And
Jonathan Huggins\footnote[1]{}\\
Department of EECS \\
Massachusetts Institute of Technology \\
Cambridge, MA 02139 \\
\texttt{jhuggins@mit.edu}
}

\newcommand{\fix}{\marginpar{\textcolor{NAColor}{FIX}}}
\newcommand{\new}{\marginpar{\textcolor{NAColor}{NEW}}}
\allowdisplaybreaks

\nipsfinalcopy

\begin{document}
\maketitle

\paragraph{Introduction.} 
Recent work has shown many connections between conditional gradient 
and other first-order optimization methods, such as herding \cite{Bach:2012a} 
and subgradient descent \cite{Bach:2012b}. By considering a type of 
\emph{proximal conditional method}, which we call boosted mirror 
descent (\bmd), we are able to unify all of these algorithms into 
a single framework, which can be interpreted as taking successive 
arg-mins of a sequence of surrogate functions. Using a standard 
online learning analysis based on Bregman divergences, we are 
able to demonstrate an $O(1/T)$ convergence rate for all algorithms 
in this class. 

\paragraph{Setup.} 
More concretely, suppose that we are given a function 
$L : U \times \Theta \to \bR$ defined by
\[ L(u, \theta) = h(u) + \ip{u}{\theta} - R(\theta) \]
and wish to find the \emph{saddle point} 
\[ L_* \eqdef \min_{u} \max_{\theta} L(u, \theta). \]
We should think of $u$ as the primal variables and $\theta$ 
as the dual variables; we will assume throughout that 
$h$ and $R$ are both convex. We will also abuse notation 
and define $L(u) \eqdef \max_{\theta} L(u, \theta)$; we 
can equivalently write $L(u)$ as 
\[ L(u) = h(u) + R^*(u), \]
where $R^*$ is the Fenchel conjugate of $R$. Note that 
$L(u)$ is a convex function. Moreover, since 
$R \leftrightarrow R^*$ is a one-to-one mapping, for 
\emph{any} convex function $L$ and \emph{any} decomposition 
of $L$ into convex functions $h$ and $R^*$, we get a 
corresponding two-argument function $L(u, \theta)$. 

\paragraph{Primal algorithm.}
Given the function $L(u, \theta)$, we define the following 
optimization procedure, which will generate a sequence of 
points $(u_1,\theta_1), (u_2, \theta_2), \ldots$ converging 
to a saddle point of $L$. First, take a sequence of weights
$\alpha_1, \alpha_2, \ldots$, and for notational convenience 
define
\(
\hat{u}_t = \frac{\sum_{s=1}^t \alpha_su_s}{\sum_{s=1}^t \alpha_s} 
\quad \text{and} \quad 
\hat{\theta}_t = \frac{\sum_{s=1}^t \alpha_s\theta_s}{\sum_{s=1}^t \alpha_s}.
\)
Then 
the {\em primal boosted mirror descent} (\primal) algorithm is the following iterative procedure:
\begin{enumerate}
\item $u_1 \in \arg\min_u h(u)$
\item $\theta_{t} \in \arg\max_{\theta \in \Theta} \langle \theta, u_t \rangle - R(\theta) = \partial R^{*}(u_{t})$
\item $u_{t+1} \in \arg\min_{u} h(u) + \langle \hat{\theta}_t, u \rangle = \partial h^{*}(-\hat\theta_{t})$
\end{enumerate}
As long as $h$ is strongly convex, for the proper choice of $\alpha_{t}$ we 
obtain the bound (see Corollary~\ref{cor:method-1}):
\begin{equation}
\sup_{\theta \in \Theta} L(\hat{u}_T, \theta) \leq L_{*} + O(1/T).
\end{equation}

As an example, suppose that we are given a $\gamma$-strongly convex function 
$f$: that is, $f(x) = \frac{\gamma}{2}\|x\|_2^2 + f_0(x)$, where $f_0$ is convex. 
Then we let $h(x) = \frac{\gamma}{2}\|x\|_2^2$, $R^*(x) = f_0(x)$, and obtain 
the updates:
\begin{enumerate}
\item $u_1 = 0$
\item $\theta_t = \partial f_0(u_t)$
\item $u_{t+1} = -\frac{1}{\gamma}\hat{\theta}_t = -\frac{\sum_{s=1}^t \alpha_s\partial f_0(u_s)}{\gamma \sum_{s=1}^t \alpha_s}$
\end{enumerate}
We therefore obtain a variant on subgradient descent where $u_{t+1}$ 
is a weighted average of the first $t$ subgradients (times a step size 
$1/\gamma$). Note that these are the subgradients of $f_0$, which are related 
to the subgradients of $f$ by $\partial f_0(x) = \partial f(x) - \gamma x$.

\paragraph{Dual algorithm.}
We can also consider the dual form of our mirror descent algorithm 
({\em dual boosted mirror descent}, or \dual):
\begin{enumerate}
\item $\theta_1 \in \arg\min_{\theta} R(\theta)$
\item $u_t \in \arg\min_{u} h(u) + \langle \theta_t, u \rangle = \partial h^{*}(-\theta_{t})$
\item $\theta_{t+1} \in \arg\max_{\theta \in \Theta} \langle \theta, \hat{u}_t \rangle - R(\theta) = \partial R^{*}(\hat u_{t})$
\end{enumerate}
Convergence now hinges upon strong convexity of $R$ rather than 
$h$, and we obtain the same $O(1/T)$ convergence 
rate (see Corollary~\ref{cor:method-2}):
\[ \sup_{\theta \in \Theta} L(\hat{u}_T, \theta) \leq L_{*} + O(1/T). \]
An important special case is $h(u) = \left\{ \begin{array}{ccl} 0 & : & u \in K \\ \infty & : & u \not\in K \end{array} \right.$, where $K$ is some 
compact set. Also let $R^* = f$, where $f$ is an arbitrary strongly convex 
function. Then we are minimizing $f$ over the compact set $K$, and we obtain 
the following updates, which are equivalent to (standard) conditional gradient 
or Frank-Wolfe:
\begin{enumerate}
\item $\theta_1 = \partial f(0)$
\item $u_t \in \arg\min_{u \in K} \langle \theta_t, u \rangle$
\item $\theta_{t+1} = \partial f(\hat{u}_t)$
\end{enumerate}
Our notation is slightly different from previous presentations in 
that we use linear weights ($\alpha_t$) instead of geometric weights 
(often denoted $\rho_t$, as in \cite{Bach:2012b}). However, under the 
mapping $\alpha_t = \rho_t/\prod_{s=1}^t (1-\rho_s)$, we obtain an 
algorithm equivalent to the usual formulation.

\paragraph{Optimality.}
The solutions generated by conditional gradient have 
an attractive sparsity property: the solution after the $t$-th iteration is 
a convex combination of $t$ extreme points of the optimized space. In 
\cite{Jaggi:2013} it is shown that conditional gradient is asymptotically 
optimal amongst algorithms with sparse solutions. This 
suggests that the $O(1/T)$ convergence rate obtained in this paper cannot be 
improved without further assumptions. For further discussion of the properties 
of conditional gradient, see Jaggi~\cite{Jaggi:2013}.

\paragraph{Primal vs. dual algorithms.}
It is worth taking a moment to contrast the primal and dual algorithms. Note 
that \emph{both} algorithms have a primal convergence guarantee (i.e. both 
guarantee that $\hat{u}_T$ is close to optimal). The sense in which the latter 
algorithm is a ``dual'' algorithm is that it hinges on strong convexity of 
the dual, as opposed to strong convexity of the primal. If we care about dual 
convergence instead of primal convergence, the only change we need to make 
is to take $\hat{\theta}_T$ at the end of the algorithm instead of $\hat{u}_T$. 
(This can be seeing by defining the loss function $L'(\theta, u) \eqdef -L(u, \theta)$, 
which inverts the role of $u$ and $\theta$ but yields the same updates as above.)

\paragraph{Discussion.}
Our framework is intriguing in that it involves a purely greedy minimization 
of surrogate loss functions (alternating between the primal and dual), and yet 
is powerful enough to capture both (a variant of) subgradient descent and 
conditional gradient descent, as well as a host of other first-order methods. 

An example of a more complex first-order method captured by our framework is 
the low-rank SDP solver introduced by Arora, Hazan, and Kale \cite{aroraSDP}. 
Briefly, the AHK algorithm seeks to minimize 
$\sum_{j=1}^m \frac{1}{2}(\Tr(A_j^TX)-b_j)^2$ subject to the 
constraints $X \succeq 0$ and $\Tr(X) \leq \rho$.\footnote{This is actually a 
variant of their algorithm, which we present for ease of exposition.} We 
can then define 
\[ h(X) = \left\{ \begin{array}{ccl} 0 & : & X \succeq 0\mathrm{ \ and \ }\Tr(X) \leq \rho \\ \infty & : & \mathrm{else} \end{array} \right.\]
and
\[ R^*(X) = \sum_{j=1}^m \frac{1}{2}(\Tr(A_j^TX)-b_j)^2. \]
Note that this decomposition is actually a special case of the conditional 
gradient decomposition above, and so we obtain the updates 
\[ X_{t+1} \in \operatorname{argmin}_{X \succeq 0, Tr(X) \leq \rho} \sum_{j=1}^m \left[\Tr(A_j^T\hat{X}_t)-b_j\right]\Tr(A_j^TX), \]
whose solution turns out to be $\rho vv^T$, where $v$ is the top singular 
vector of the matrix
$-\sum_{j=1}^m \left[\Tr(A_j^T\hat{X}_t)-b_j\right]A_j$. This example serves both 
to illustrate the flexibility of our framework and to highlight the interesting 
fact that the Arora-Hazan-Kale SDP algorithm is a special case of conditional 
gradient (to get the original formulation in \cite{aroraSDP}, we need to replace 
the function $\frac{1}{2}x^2$ with $x_{+}\log x_{+}$, where $x_{+} = \max(x,0)$). 
It is worth noting that the analysis in \cite{aroraSDP} appears to be tighter in 
their special case than the analysis we give here. We plan to investigate this 
phenomenon in future work.

\paragraph{Related work.}
A generalized Frank-Wolfe algorithm has also been proposed in \cite{Mairal:2013}, 
which considers first-order convex surrogate functions more generally. Algorithm 3 
of \cite{Mairal:2013} turns out to be equivalent to a line-search formulation of the 
algorithm we propose in this paper. However, the perspective is quite different. 
While both algorithms share the intuition of trying to construct a ``proximal Frank-Wolfe'' 
algorithm, the general framework of \cite{Mairal:2013} studies convex \emph{upper bounds}, 
rather than the convex \emph{lower bounds} of this paper (although for the 
``proximal gradient surrogates'' that they suggest using, their Frank-Wolfe algorithm 
does implicitly use lower bounds). Another difference is that their analysis focuses 
on the Lipschitz properties of the objective function, whereas we use the more general 
notion of strong convexity with respect to an arbitrary norm. We believe that this 
geometric perspective is useful for tailoring the proximal function to specific applications, 
as demonstrated in our discussion of $q$-herding below. Finally, the saddle point 
perspective on Frank-Wolfe is not present in \cite{Mairal:2013}, although they 
do discuss the general idea of saddle-point methods in relation to convex surrogates.



\paragraph{$q$-herding.}
In addition to unifying several existing methods, our framework allows us to 
extend herding to an algorithm we call \emph{$q$-herding}. Herding is 
an algorithm for constructing pseudosamples that match a specified collection 
of moments from a distribution; it was originally introduced by Welling 
\cite{Welling:2009a} and was shown to be a special case of conditional gradient 
by Bach et al.~\cite{Bach:2012a}. Let $\sM_{\sX}$ be the probability simplex over 
$\sX$. Herding can be cast as trying to minimize 
$\|\bE_{\mu}[\phi(x)]-\bar{\phi}\|_2^2$ over $\mu \in \sM_{\sX}$, for a
given $\phi : \sX \to \bR^d$ and $\bar{\phi} \in \bR^d$. As shown in 
\cite{Bach:2012a}, the herding updates are equivalent to \dual with 
$h(\mu) \equiv 0$ and $R(\theta) = \theta^T\bar{\phi} + \frac{1}{2}\|\theta\|_2^2$.

The implicit assumption in the herding algorithm is that $\|\phi(x)\|_2$ is 
bounded. We are able to construct a more general algorithm that only requires 
$\|\phi(x)\|_p$ to be bounded for some $p \geq 2$. This \emph{$q$-herding} 
algorithm works by taking 
$R(\theta) = \theta^T\bar{\phi} + \frac{1}{q}\|\theta\|_q^q$, where 
$\frac{1}{p} + \frac{1}{q} = 1$. In this case our convergence results imply that 
$\|\bE_{\mu}[\phi(x)]-\bar{\phi}\|_p^p$ decays at a rate of $O(1/T)$ (proof to 
appear in an extended version of this paper).

\paragraph{Convergence results.}
We end by stating our formal convergence results. Proofs are given in the Appendix. For the primal algorithm 
(\primal) we have:
\begin{theorem}
\label{thm:method-1}
Suppose that $h$ is strongly convex with respect to a norm $\|\cdot\|$ 
and let $r = \sup_{\theta} \|\theta\|_{*}$. Then, for any $u^*$,
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2}. \]
\end{theorem}
\begin{corollary} 
\label{cor:method-1}
Under the hypotheses of Theorem \ref{thm:method-1}, for $\alpha_{t} = 1$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2 (\log (T) + 1)}{T}. \]
and for $\alpha_t = t$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{8r^2}{T}. \]
\end{corollary}
Similarly, for the dual algorithm (\dual) we have:
\begin{theorem}
\label{thm:method-2}
Suppose that $R$ is strongly convex with respect to a norm $\|\cdot\|$ 
and let $r = \sup_{u} \|u\|_{*}$. Then, for any $u^*$,
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2}. \]
\end{theorem}
\begin{corollary}
\label{cor:method-2}
Under the hypotheses of Theorem \ref{thm:method-2}, for $\alpha_t = 1$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2(\log(T) + 1)}{T} \]
and for $\alpha_t = t$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{8r^2}{T} \]
\end{corollary}
Thus, a step size of $\alpha_t = t$ yields the claimed $O(1/T)$ convergence rate.

\paragraph{Acknowledgements.}
Thanks to Cameron Freer, Simon Lacoste-Julien, Martin Jaggi, Percy Liang, Arun Chaganty, and Sida Wang 
for helpful discussions. Thanks also to the anonymous reviewer for suggesting improvements 
to the paper. JS was supported by the Hertz Foundation. JHH was supported by the U.S. Government under FA9550-11-C-0028 and awarded by the Department of Defense, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a.

\bibliography{herding}
\bibliographystyle{plain}

\newpage 

\section*{Appendix: Convergence Proofs} 
\label{sec:proofs}

We now prove the convergence results given above. Throughout this section, assume that $\alpha_1,\ldots,\alpha_T$ is 
a sequence of real numbers and that $A_t = \sum_{s=1}^t \alpha_s$. 
We further require that $A_t > 0$ for all $t$.

Also recall that the Bregman divergence is defined by 
$D_f(x_2 \| x_1) \eqdef f(x_2) - \langle \partial f(x_1), x_2-x_1 \rangle - f(x_1)$.

Our proofs hinge on the following key lemma:

%%% MAIN LEMMA %%%
\begin{lemma}
\label{lem:bregman}
Let $z_1,\ldots,z_T$ be vectors and let $f(x)$ be a strictly convex 
function. Define $\hat{z}_t$ to be $\frac{1}{A_t} \sum_{s=1}^t \alpha_s z_s$.
Let $x_1,\ldots,x_T$ be chosen via $x_{t+1} = \arg\min_{x} f(x) + \langle \hat{z}_t, x\rangle$. 
Then for any $x^*$ we have
\begin{align*}
\frac{1}{A_T} \sum_{t=1}^T \{\alpha_t(f(x_t) + \langle z_t, x_t \rangle)\} \leq f(x^*) + \langle \hat{z}_t, x^* \rangle + \frac{1}{A_T} \sum_{t=1}^T A_t D_{f}(x_t \| x_{t+1}). 
% \lefteqn{\frac{1}{A_T} \sum_{t=1}^T \{\alpha_t(f(x_t) + \langle z_t, x_t \rangle)\}} \\
% \phantom{+} &\leq f(x^*) + \langle \hat{z}_t, x^* \rangle + \frac{1}{A_T} \sum_{t=1}^T A_t D_{f}(x_t \| x_{t+1}). 
\end{align*}
\end{lemma}

%%% START PROOF %%%
\begin{proof}
First note that, if $x_0 = \arg\min f(x) + \langle z, x \rangle$, 
then $\partial f(x_0) = -z$.

Now note that
\begin{align}
\alpha_{t}z_{t} 
&= A_{t}\hat z_{t} - A_{t-1}\hat z_{t-1} \\
&= - A_{t}\partial f(x_{t+1}) + A_{t-1} \partial f(x_{t}),
\end{align}
so we have
\begin{align} 
\lefteqn{\sum_{t=1}^T \{\alpha_t(f(x_t) + \langle z_t, x_t \rangle)\}} \\
 &= \sum_{t=1}^T \{\alpha_t f(x_t) + \langle A_{t-1} \partial f(x_t) - A_t \partial f(x_{t+1}), x_t \rangle\} \\
 &= \sum_{t=1}^T \{\alpha_t f(x_t) - \langle A_{t} \partial f(x_{t+1}), x_t-x_{t+1} \rangle\} \\
 & \quad - A_{T}\langle \partial f(x_{T+1}), x_{T+1} \rangle \\
 &= \sum_{t=1}^T \{A_t f(x_t) - \langle A_{t} \partial f(x_{t+1}), x_t-x_{t+1} \rangle - A_t f(x_{t+1})\}  \\
 &\quad+ A_T(f(x_{T+1}) - \langle \partial f(x_{T+1}), x_{T+1} \rangle) \nonumber \\ 
 &= \sum_{t=1}^T \{A_tD_f(x_t \| x_{t+1})\}  \\ 
 &\quad+ A_T(f(x_{T+1}) - \langle \partial f(x_{T+1}), x_{T+1} \rangle) \nonumber \\
 &= \sum_{t=1}^T \{A_tD_f(x_t \| x_{t+1})\} + A_T(f(x_{T+1}) + \langle \hat{z}_T, x_{T+1} \rangle) \\
 &\leq \sum_{t=1}^T \{A_tD_f(x_t \| x_{t+1})\} + A_T(f(x^*) + \langle \hat{z}_T, x^* \rangle). 
\end{align}
Dividing both sides by $A_T$ completes the proof.
\end{proof}
%%% END PROOF %%%

We also note that $D_f(x_t \| x_{t+1}) = D_{f^*}(\hat{z}_{t+1} \| z_t)$, where $f^*(z) = \sup_x \langle z,x\rangle - f(x)$. 
This form of the bound will often be more useful to us. Another useful property of Bregman divergences is the following lemma 
which relates strong convexity of $f$ to strong smoothness of $f^*$:
\begin{lemma}
\label{lem:convexity}
Suppose that $D_f(x' \| x) \geq \frac{1}{2}\|x-x'\|^2$ for some 
norm $\|\cdot\|$ and for all $x, x'$. (In this case we say that $f$ is strongly 
convex with respect to $\|\cdot\|$.) 
Then $D_{f^*}(y' \| y) \leq \frac{1}{2}\|y-y'\|_{*}^2$ for all $y, y'$.
\end{lemma}

%%% ALGORITHM 1 ANALYSIS %%%
We require a final supporting proposition before proving Theorem \ref{thm:method-1}. 

% Convergence proposition
\begin{proposition}[Convergence of \primal]
\label{prop:method-1}
Consider the updates $\theta_t \in \arg\max_{\theta} \langle \theta, u_t \rangle - R(\theta)$ 
and $u_{t+1} \in \arg\min_u h(u) + \langle \hat{\theta}_s, u \rangle$. 
Then we have
\begin{equation}
\sup_{\theta} L(\hat{u}_T, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_h(u_t \| u_{t+1}).
\end{equation}
\end{proposition}
\begin{proof}
Note that $L(u_t, \theta_t) = \max_{\theta} L(u_t, \theta)$ by construction. 
Also note that, if we invoke Lemma~\ref{lem:bregman} with $f = h$ and 
$z_t = \theta_t$ then we get the inequality
\begin{align}
\frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) 
&\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u^*, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_tD_h(u_t \| u_{t+1}).
\end{align}
Combining these together, we get the string of inequalities
\begin{align*} 
L(\hat{u}_T, \theta) &= L\left(\frac{1}{A_T} \sum_{t=1}^T \alpha_tu_t, \theta\right) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u^*, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_t D_h(u_t \| u_{t+1}) \\
 &\leq \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_h(u_t \| u_{t+1}),
\end{align*}
as was to be shown.
\end{proof}

% Convergence rate theorem
%\begin{theorem}
%\label{thm:method-1}
%Suppose that $h$ is strongly convex with respect to a norm $\|\cdot\|$ 
%and let $r = \sup_{\theta} \|\theta\|_{*}$. Then 
%\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2}. \]
%\end{theorem}
\begin{proof}[Proof of Theorem \ref{thm:method-1}]
By Lemma~\ref{lem:convexity}, we have 
\begin{align*} 
D_h(u_t \| u_{t+1}) &= D_{h^*}(\hat{\theta}_{t+1} \| \hat{\theta}_{t}) \\
 &\leq \frac{1}{2}\|\hat\theta_{t+1}-\hat\theta_{t}\|_{*}^2 \\
 &= \frac{1}{2}\|\frac{\sum_{s \leq t}\alpha_s\theta_s}{\sum_{s \leq t} \alpha_s} - \frac{\sum_{s \leq t+1}\alpha_s\theta_s}{\sum_{s \leq t+1} \alpha_s}\|_{*}^2 \\
 &= \frac{1}{2}\|\frac{\alpha_{t+1}}{A_tA_{t+1}} \sum_{s \leq t} \alpha_s\theta_s - \frac{\alpha_{t+1}}{A_{t+1}} \theta_{t+1}\|_{*}^2 \\
 &\leq \frac{1}{2} \left(\frac{\alpha_{t+1}}{A_tA_{t+1}} \sum_{s \leq t} \alpha_s\|\theta_s\|_{*}^2 + \frac{\alpha_{t+1}}{A_{t+1}} \|\theta_{t+1}\|_{*}^2\right)^2 \\
 &\leq \frac{2r^2\alpha_{t+1}^2}{A_{t+1}^2}.
\end{align*}
It follows that 
\begin{align*}
\frac{1}{A_T} \sum_{t=1}^T A_tD_h(u_t \| u_{t+1}) &\leq \frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2}.
\end{align*}
\end{proof}

% Convergence rate corollary 
%\begin{corollary} 
%\label{cor:method-1}
%Under the hypotheses of Theorem \ref{thm:method-1}, for $\alpha_{t} = 1$ we have
%\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2 (\log (T) + 1)}{T}. \]
%and for $\alpha_t = t$ we have
%\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{8r^2}{T}. \]
%\end{corollary}
\begin{proof}[Proof of Corollary \ref{cor:method-1}]
If we let $\alpha_t = 1$, then $A_t = t$ and $\frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} = \frac{t}{(t+1)^{2}} \le \frac{1}{t}$.
We therefore get
\begin{equation}
\frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} \leq \frac{2r^2}{T} \sum_{t=1}^{T}\frac{1}{t} \leq \frac{2r^2(\log (T) + 1)}{T+1}.
\end{equation}
If we let $\alpha_t = t$, then $A_t = \frac{t(t+1)}{2}$ and 
$\frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} = \frac{2(t+1)^2t(t+1)}{(t+1)^2(t+2)^2} = \frac{2t(t+1)}{(t+2)^2} \leq 2$.
We therefore get
\begin{equation}
\frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} \leq \frac{4r^2}{T(T+1)} \sum_{t=1}^{T}2 = \frac{8r^2}{T+1},
\end{equation}
which completes the proof.
\end{proof}

%%% ALGORITHM 2 ANALYSIS %%%
The proof of Theorem \ref{thm:method-2} requires an analagous supporting proposition to that of Theorem \ref{thm:method-1}.
% Convergence proposition
\begin{proposition}[Convergence of \dual]
\label{prop:method-2}
Consider the updates $u_t \in \arg\min_{u} h(u) + \langle \theta_t, u \rangle$ 
and $\theta_{t+1} \in \arg\max_{\theta} \langle \theta, \hat{u}_t \rangle - R(\theta)$. 
Then we have 
\begin{equation}
\sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_{R}(\theta_t \| \theta_{t+1}).
\end{equation}
\end{proposition}
\begin{proof}
If we invoke Lemma~\ref{lem:bregman} with $f=R$ and $z_t=-u_t$, then for all $\theta$ we get 
the inequality
\begin{align}
\frac{1}{A_T} \sum_{t=1}^T -\alpha_t L(u_t, \theta_t) 
&\leq \frac{1}{A_T} \sum_{t=1}^T -\alpha_t L(u_t, \theta) \\
&\quad - \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}). \nonumber
\end{align}
Re-arranging yields
\begin{align}
\frac{1}{A_T }\sum_{t=1}^T \alpha_t L(u_t, \theta) 
&\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) \\
&\quad+ \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}). 
\end{align}
Now, we have the following string of inequalities:
\begin{align*} 
L(\hat{u}, \theta) &= L\left(\frac{1}{A_T} \sum_{t=1}^T \alpha_t u_t, \theta\right) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_t D_R(\theta_t \| \theta_{t+1}) \\
 &= \frac{1}{A_T} \sum_{t=1}^T \alpha_t \inf_{u} L(u, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u^*, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}) \\
 &= \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}),
\end{align*}
as was to be shown.
\end{proof}

\begin{proof}[Proofs of Theorem~\ref{thm:method-2} and Corollary~\ref{cor:method-2}]
The proofs are identical to those for \primal.
\end{proof}

\end{document}
