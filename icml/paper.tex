%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2013 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{import, icml2013}
\input{latex-defs.tex}

\icmltitlerunning{Boosted Mirror Descent}

\begin{document} 

\twocolumn[
\icmltitle{Boosted Mirror Descent, Frank-Wolfe, and Herding}

\icmlauthor{Jacob Steinhardt}{jsteinhardt@cs.stanford.edu}
\icmladdress{Stanford University,
             353 Serra Street, Stanford, CA 94305 USA}
\icmlauthor{Jonathan Huggins}{jhuggins@mit.edu}
\icmladdress{Massachusetts Institute of Technology,
             43 Vassar Street, Cambridge, MA 02139 USA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boosting, optimization, mirror descent, frank-wolfe, herding}

\vskip 0.3in
]

\begin{abstract} 
TODO
\end{abstract} 

\section{Introduction}
\label{sec:intro}

\section{Boosted Mirror Descent}
\label{sec:algorithm}

\NA{JHH: I'm thinking that from the beginning we should let $u$ be defined over a different space than $\theta$ and assume a linear map $u \mapsto \eta(u) \in \Theta$. Then write $\langle \theta, \eta(u) \rangle$ so the setup will align with the herding case, where $\eta(u) = \bE_{u}[\phi(x)]$. }

Consider the following loss function:
\begin{equation}
L(u) = h(u) + \max_{\theta \in \Theta} \left\{\langle \theta, u \rangle - R(\theta) \right\}.
\end{equation}
We can think of $h$ as a \emph{primal regularizer} and $R$ as a \emph{dual regularizer}. 

Examples:
\begin{itemize}
\item Let $h(u) = 0$ and $R(\theta) = \langle \theta, u_0 \rangle$. Then $L(u)$ is the 
      \emph{maximum mean discrepancy} between $u$ and $u_0$ 
      relative to $\Theta$.
\item Let $h(u) = 0$ and $R(\theta) = S^*(\theta)$ where $S^*$ is the Fenchel conjugate of 
      any strongly convex function $S$. Then $L(u) = S(u)$.
\item Let $h(u) = \|u\|_1$, $R(u) = \langle \theta, u_0 \rangle + \frac{1}{2} \|\theta\|_2^2$. 
      Then $L(u) = \|u\|_1 + \frac{1}{2} \|u-u_0\|_2^2$. 
\end{itemize}

In this paper we will consider a family of methods for minimizing $L(u)$, 
which are based on Bach's generalization of the Frank-Wolfe algorithm and can 
be interpreted as boosted mirror descent.

First, let us generalize the setting 
to consider a \emph{two-argument} loss function
\begin{equation}
L(u,\theta) = h(u) + \langle \theta, u \rangle - R(\theta).
\end{equation}
We will assume throughout that $h$ and $R$ are both convex functions, 
and furthermore that $\arg\max_{u} h(u) + \theta^Tu$ can be efficiently 
computed for all values of $\theta$.

Mirror descent, together with boosting, yields an algorithm for finding 
``saddle points'' of $L$. It is the following algorithm (called Algorithm 1).
For notational convenience, for a sequence of weights $\alpha_1,\alpha_2,\ldots$ 
let $\hat{u}_t = \frac{\sum_{s=1}^t \alpha_su_s}{\sum_{s=1}^t \alpha_s}$ and let 
$\hat{\theta}_t = \frac{\sum_{s=1}^t \alpha_s\theta_s}{\sum_{s=1}^t \alpha_s}$.
\begin{enumerate}
\item $u_1 \in \arg\min_u h(u)$
\item $\theta_{t} \in \arg\max_{\theta \in \Theta} \langle \theta, u_t \rangle - R(\theta)$
\item $u_{t+1} \in \arg\min_{u} h(u) + \langle \hat{\theta}_t, u \rangle$
%\item $u_{t+1} \in \arg\min_{u} h(u) + \langle \frac{1}{t} \sum_{s \leq t} \theta_s, u \rangle$
\end{enumerate}
As long as $h$ is strongly convex, we obtain the 
bound (see Corollary~\ref{cor:method-1}):
\begin{equation}
\sup_{\theta \in \Theta} L(\hat{u}_T, \theta) \leq \sup_{\theta \in \Theta} L(u^*, \theta) + O(1/T).
\end{equation}
In other words, $\hat{u}_T$ is close to being a global minimum of $L$.
However, it is often the case that $h$ is not strongly convex, whereas $R$ is strongly convex. In this case 
we may wish to use the following slightly different algorithm (called Algorithm 2):
\begin{enumerate}
\item $\theta_1 \in \arg\min_{\theta} R(\theta)$
\item $u_t \in \arg\min_{u} h(u) + \langle \theta_t, u \rangle$
%\item $\theta_{t+1} \in \arg\max_{\theta \in \Theta} \langle \theta, \frac{1}{t} \sum_{s \leq t} u_s \rangle - R(\theta)$
\item $\theta_{t+1} \in \arg\max_{\theta \in \Theta} \langle \theta, \hat{u}_t \rangle - R(\theta)$
\end{enumerate}
We then obtain the following bound 
when $R$ is strongly convex (see Corollary~\ref{cor:method-2}):
\[ \sup_{\theta \in \Theta} L(\hat{u}_T, \theta) \leq \sup_{\theta \in \Theta} L(u^*, \theta) + O(1/T). \]
Algorithm 1 and Algorithm 2 are closely related; indeed, they are dual to each other 
(performing Algorithm 1 on $L(u,\theta)$ is the same as performing Algorithm 2 on 
$-L(\theta,u)$).

\section{Convergence Proofs}
\label{sec:proofs}
We now prove the convergence results cited in Section~\ref{sec:algorithm}. 
Throughout this section, assume that $\alpha_1,\ldots,\alpha_T$ is 
a sequence of real numbers and that $A_t = \sum_{s=1}^t \alpha_s$. 
We further require that $A_t \geq 0$ for all $t$.

Also recall that the Bregman divergence is defined by 
$D_f(x_2 \| x_1) \eqdef f(x_2) - \langle \nabla f(x_1), x_2-x_1 \rangle - f(x_1)$.

Our proofs hinge on the following key lemma:
%%% MAIN LEMMA %%%
\begin{lemma}
\label{lem:bregman}
Let $z_1,\ldots,z_T$ be vectors and let $f(x)$ be a strictly convex 
function. Define $\hat{z}_t$ to be $\frac{1}{A_t} \sum_{s=1}^t \alpha_s z_s$.

Let $x_1,\ldots,x_T$ be chosen via $x_{t+1} = \arg\min_{x} f(x) + \langle \hat{z}_t, x\rangle$. 
Then for any $x^*$ we have
\begin{align*}
\lefteqn{\frac{1}{A_T} \sum_{t=1}^T \{\alpha_t(f(x_t) + \langle z_t, x_t \rangle)\}} \\
\phantom{+} &\leq f(x^*) + \langle \hat{z}_t, x^* \rangle + \frac{1}{A_T} \sum_{t=1}^T A_t D_{f}(x_t \| x_{t+1}). 
\end{align*}
\end{lemma}
\begin{proof}
First note that, if $x_0 = \arg\min f(x) + \langle z, x \rangle$, 
then $\nabla f(x_0) = -z$.

\NA{JNS: In the following proof, (2) is by the update formula for $x_{t+1}$ together with the preceding observation.  (3) is applying (2). (4) and (5) are re-arranging the sum. (6) is definition of Bregman divergence. (7) is the observation again. 
(8) is the definition of $x_{T+1}$.}

Now note that
\begin{align}
\alpha_{t}z_{t} 
&= A_{t}\hat z_{t} - A_{t-1}\hat z_{t-1} \\
&= - A_{t}\nabla f(x_{t+1}) + A_{t-1} \nabla f(x_{t}),
\end{align}
so we have
\begin{align}
\lefteqn{\sum_{t=1}^T \{\alpha_t(f(x_t) + \langle z_t, x_t \rangle)\}} \\
 &= \sum_{t=1}^T \{\alpha_t f(x_t) + \langle A_{t-1} \nabla f(x_t) - A_t \nabla f(x_{t+1}), x_t \rangle\} \\
 &= \sum_{t=1}^T \{\alpha_t f(x_t) - \langle A_{t} \nabla f(x_{t+1}), x_t-x_{t+1} \rangle\} \\
 & \quad - A_{T}\langle \nabla f(x_{T+1}), x_{T+1} \rangle
\end{align}
\begin{align}
 &= \sum_{t=1}^T \{A_t f(x_t) - \langle A_{t} \nabla f(x_{t+1}), x_t-x_{t+1} \rangle - A_t f(x_{t+1})\}  \\
 &\quad+ A_T(f(x_{T+1}) - \langle \nabla f(x_{T+1}), x_{T+1} \rangle) \nonumber \\
 &= \sum_{t=1}^T \{A_tD_f(x_t \| x_{t+1})\}  \\
 &\quad+ A_T(f(x_{T+1}) - \langle \nabla f(x_{T+1}), x_{T+1} \rangle) \nonumber \\
 &= \sum_{t=1}^T \{A_tD_f(x_t \| x_{t+1})\} + A_T(f(x_{T+1}) + \langle \hat{z}_T, x_{T+1} \rangle) \\
 &\leq \sum_{t=1}^T \{A_tD_f(x_t \| x_{t+1})\} + A_T(f(x^*) + \langle \hat{z}_T, x^* \rangle). 
\end{align}
Dividing both sides by $A_T$ completes the proof.
\end{proof}
We also note that $D_f(x_t \| x_{t+1}) = D_{f^*}(\hat{z}_{t+1} \| z_t)$, where $f^*(z) = \sup_x \langle z,x\rangle - f(x)$. 
This form of the bound will often be more useful to us.
\begin{lemma}
\label{lem:convexity}
Suppose that $D_f(x' \| x) \geq \frac{1}{2}\|x-x'\|^2$ for some 
norm $\|\cdot\|$. (In this case we say that $f$ is strongly 
convex with respect to $\|\cdot\|$.) 
Then $D_{f^*}(x' \| x) \leq \frac{1}{2}\|x-x'\|_{*}^2$.
\end{lemma}

%%% ALGORITHM 1 ANALYSIS %%%
% Convergence proposition
\begin{proposition}[Convergence of Algorithm 1]
\label{prop:method-1}
Consider the updates $\theta_t \in \arg\max_{\theta} \langle \theta, u_t \rangle - R(\theta)$ 
and $u_{t+1} \in \arg\min_u h(u) + \langle \hat{\theta}_s, u \rangle$. 
Then we have
\begin{equation}
\sup_{\theta} L(\hat{u}_T, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_f(x_t \| x_{t+1}).
\end{equation}
\end{proposition}
\begin{proof}
Note that $L(u_t, \theta_t) = \max_{\theta} L(u_t, \theta)$ by construction. 
Also note that, if we invoke Lemma~\ref{lem:bregman} with $f = h$ and 
$z_t = \theta_t$, then we get the inequality
\begin{align}
\frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) 
&\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u^*, \theta_t) \\
&\quad + \frac{1}{A_T} \sum_{t=1}^T A_tD_f(x_t \| x_{t+1}). \nonumber
\end{align}
Combining these together, we get the string of inequalities
\begin{align*}
L(\hat{u}_T, \theta) &= L\left(\frac{1}{A_T} \sum_{t=1}^T \alpha_tu_t, \theta\right) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u^*, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_t D_f(u_t \| u_{t+1}) \\
 &\leq \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_f(u_t \| u_{t+1}),
\end{align*}
as was to be shown.
\end{proof}

% Convergence rate theorem
\begin{theorem}
\label{thm:method-1}
Suppose that $h$ is strongly convex with respect to a norm $\|\cdot\|$ 
and let $r = \sup_{\theta} \|\theta\|_{*}$. Then 
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2}. \]
\end{theorem}
\begin{proof}
By Lemma~\ref{lem:convexity}, we have 
\begin{align*}
D_f(x_t \| x_{t+1}) &= D_{f^*}(\hat{\theta}_{t+1} \| \hat{\theta}_{t}) \\
 &\leq \frac{1}{2}\|\hat\theta_{t+1}-\hat\theta_{t}\|_{*}^2 \\
 &= \frac{1}{2}\|\frac{\sum_{s \leq t}\alpha_s\theta_s}{\sum_{s \leq t} \alpha_s} - \frac{\sum_{s \leq t+1}\alpha_s\theta_s}{\sum_{s \leq t+1} \alpha_s}\|_{*}^2 \\
 &= \frac{1}{2}\|\frac{\alpha_{t+1}}{A_tA_{t+1}} \sum_{s \leq t} \alpha_s\theta_s - \frac{\alpha_{t+1}}{A_{t+1}} \theta_{t+1}\|_{*}^2 \\
 &\leq \frac{1}{2} \left(\frac{\alpha_{t+1}}{A_tA_{t+1}} \sum_{s \leq t} \alpha_s\|\theta_s\|_{*}^2 + \frac{\alpha_{t+1}}{A_{t+1}} \|\theta_{t+1}\|_{*}^2\right)^2 \\
 &\leq \frac{2r^2\alpha_{t+1}^2}{A_{t+1}^2}.
\end{align*}
It follows that 
\begin{align*}
\frac{1}{A_T} \sum_{t=1}^T A_tD_f(x_t \| x_{t+1}) &\leq \frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2}.
\end{align*}
\end{proof}

% Convergence rate corollary 
\begin{corollary} 
\label{cor:method-1}
Under the hypotheses of Theorem \ref{thm:method-1}, for $\alpha_{t} = 1$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2 (\log (T) + 1)}{T}. \]
and for $\alpha_t = t$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{8r^2}{T}. \]
\end{corollary}
\begin{proof}
If we let $\alpha_t = 1$, then $A_t = t$ and $\frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} = \frac{t}{(t+1)^{2}} \le \frac{1}{t}$.
We therefore get
\begin{equation}
\frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} \leq \frac{2r^2}{T} \sum_{t=1}^{T}\frac{1}{t} \leq \frac{2r^2(\log (T) + 1)}{T+1}.
\end{equation}
If we let $\alpha_t = t$, then $A_t = \frac{t(t+1)}{2}$ and 
$\frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} = \frac{2(t+1)^2t(t+1)}{(t+1)^2(t+2)^2} = \frac{2t(t+1)}{(t+2)^2} \leq 2$.
We therefore get
\begin{equation}
\frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} \leq \frac{4r^2}{T(T+1)} \sum_{t=1}^{T}2 = \frac{8r^2}{T+1},
\end{equation}
which completes the proof.
\end{proof}

%%% ALGORITHM 2 ANALYSIS %%%
% Convergence proposition
\begin{proposition}[Convergence of Algorithm 2]
\label{prop:method-2}
Consider the updates $u_t \in \arg\min_{u} h(u) + \langle \theta_t, u \rangle$ 
and $\theta_{t+1} \in \arg\max_{\theta} \langle \theta, \hat{u}_t \rangle - R(\theta)$. 
Then we have 
\begin{equation}
\sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_{R}(\theta_t \| \theta_{t+1}).
\end{equation}
\end{proposition}
\begin{proof}
If we invoke Lemma~\ref{lem:bregman} with $f=R$ and $z_t=-u_t$, then we get 
the inequality
\begin{align}
\frac{1}{A_T} \sum_{t=1}^T -\alpha_t L(u_t, \theta_t) 
&\leq \frac{1}{A_T} \sum_{t=1}^T -\alpha_t L(u_t, \theta^*) \\
&\quad - \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}). \nonumber
\end{align}
Re-arranging yields
\begin{align}
\frac{1}{A_T }\sum_{t=1}^T \alpha_t L(u_t, \theta^*) 
&\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) \\
&\quad+ \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}). 
\end{align}
Now, we have the following string of inequalities:
\begin{align*}
L(\hat{u}, \theta) &= L\left(\frac{1}{A_T} \sum_{t=1}^T \alpha_t u_t, \theta\right) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_t D_R(\theta_t \| \theta_{t+1}) \\
 &= \frac{1}{A_T} \sum_{t=1}^T \alpha_t \inf_{u} L(u, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u^*, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}) \\
 &= \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}),
\end{align*}
as was to be shown.
\end{proof}

% Convergence rate theorem
\begin{theorem}
\label{thm:method-2}
Suppose that $R$ is strongly convex with respect to a norm $\|\cdot\|$ 
and let $r = \sup_{u} \|u\|_{*}$. Then 
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2}. \]
\end{theorem}
\begin{corollary}
\label{cor:method-2}
Under the hypotheses of Theorem \ref{thm:method-2}, for $\alpha_t = 1$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2(\log(T) + 1)}{T} \]
and for $\alpha_t = t$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{8r^2}{T} \]
\end{corollary}
\begin{proof}[Proofs]
The proofs are identical to those for Algorithm 1.
\end{proof}

\section{Application to Herding}
\label{sec:herding}

\NA{JHH: Need to specify the spaces we are working with here.}
Suppose we are given a family of features $\phi$ together 
with a known value $\bar{\phi}$ for $\bE_{u}[\phi]$. We 
would like to construct a distribution that approximately 
matches this distribution, i.e.~one for which 
$\bE_{\hat{u}}[\phi] \approx \bar{\phi}$. A natural way 
to do this is by using boosted mirror descent to minimize the 
maximum mean discrepancy relative to $\phi$; 
in other words, to minimize
\[ L(u) = \max_{i \in \{1,\dots,d\}} |\bE_{u}[\phi_i]-\bar{\phi}_i|. \]
If we wish to write this more similarly to the original problem 
formulation, we can write
\[ L(u) = \sup_{\|\theta\|_1 \leq 1} \theta^T(\bE_{u}[\phi]-\bar{\phi}). \]
One issue is that $\|\theta\|_1$ is not strongly convex, so we relax $\|\theta\|_1$ 
to $\|\theta\|_2$ and write our loss function in the Lagrangian form:
\[ L(u, \theta) = \theta^T\bE_{u}[\phi] - \theta^T\bar{\phi} - \frac{1}{2}\|\theta\|_2^2. \]
Intriguingly, Algorithm 1 and Algorithm 2 are identical to each other for this choice of $L$ 
(up to a change of index). In this case, $u_t = \delta_{x_t}$ for some $x_t$, and 
the updates are the same as the standard herding updates. Let $r = \sup_{x} \|\phi(x)\|_2$; 
then using the Algorithm 2 convergence bound gives us
\begin{align*}
\sup_{\theta} L\left(\frac{1}{T} \sum_{t=1}^T \delta_{x_t}, \theta\right) &\leq \inf_{u^*} \sup_{\theta} L(u^*, \theta) + \frac{8r^2(\log(T)+1)}{T} \\
 &= \sup_{\theta} \inf_{u^*} L(u^*, \theta) + \frac{8R^2(\log(T)+1)}{T} \\
 &= \frac{8R^2(\log(T)+1)}{T}.
\end{align*}
This is a slightly weaker version of the typical herding bound on MMD.

\section{Herding in Infinite Dimensions}
\label{sec:infinite-case}

\section{Herding and Maximum Entropy}
\label{sec:max-ent}

Let us now consider the entropic regularizer $h(u) = \tau\bE_{u}[\log u(x)]$ with 
\[ R(\theta) = \bE_{u}[\theta^T\bar{\phi}] + \left\{ \begin{array}{llc} \infty & : & \|\theta\|_1 > 1 \\ 0 & : & \|\theta\|_1 \leq 1 \end{array} \right.\]
If we use Method 1, we then end up with 
\[ u_{t}(x) \propto \exp\left\{-\frac{1}{\alpha}\overline{\theta}_{t}^T\phi(x)\right\}, \]
where $\overline{\theta}_{t} = \frac{1}{t} \sum_{s \le t} \theta_{s}$. Hence, $h(u) + \bE_{u}[\theta^T\phi(x)] - R(\theta)$ is no longer maximized at the boundary (when $\tau > 0$). We can instead consider the following sequence of distributions:
\begin{enumerate}
\item $\theta_t \in \argmax_{\|\theta\|_1 \leq 1} \theta^T (\bE_{u_t}[\phi(x)]-\bar{\phi})$
\item $u_{t+1}(x) = Z(\overline{\theta}_{t}, \tau)^{-1} \exp\left\{-\frac{1}{\tau}\overline{\theta}_{t}^T\phi(x)\right\}$
\end{enumerate}
Here $Z(\overline{\theta}_{t}, \tau) = \int\exp\left\{-\frac{1}{\tau}\overline{\theta}_{t}^T\phi(x)\right\} \dee x$. 

The entropic regularizer is not strongly convex but a more general analysis involving 
Bregman divergence can nevertheless yield a convergence bound in this case. We thus 
get something that looks like herding but actually gives us an exponential family 
type distribution. In particular, we have a Gibbs distribution with temperature $\tau$, and 
in the limit as the temperature goes to zero, we recover herding, since 
\[ \lim_{\tau \to 0} u_{t+1}(x) = \delta_{x^{*}_{t+1}}, \]
where $x^{*}_{t+1} = \argmin_{x} \overline{\theta}_{t}^T\phi(x)$. 
This fact is reminiscent of Welling's original derivation of herding, in which 
he took the zero temperature limit of the gradient descent algorithm for the optimization problem 
\[ \argmin_{u} h(u) \quad \text{s.t.}~\bE_{u}[\phi(x)] - \bar\phi = 0. \]
This optimization is quite similar to the entropic optimization considered here, which 
we can write as 
\begin{align*}
\argmin_{u}& \argmax_{|\theta\|_1 \leq 1} h(u) + \theta^{T}(\bE_{u}[\phi(x)] - \bar \phi)  \\
&= \argmin_{u} h(u) + \|\bE_{u}[\phi(x)] - \bar\phi\|_{2}. 
\end{align*}
In the entropic formulation, the expectation constraint $\bE_{u}[\phi(x)] - \bar\phi = 0$ becomes
soft, though the two optimizations are equivalent in the zero-temperature limit. 

We can view the entropic regularization
with non-zero temperature as an alternative approximation to herding. If at each step we
retain a sample $x_{t} \sim u_{t}$, then the sequence $(x_{t})_{t}$ converges to the 
herding sample path as $\tau \to 0$. So $(x_{t})_{t}$ with non-zero temperature approximate
herding samples. \NA{JHH: I'm not sure how useful this all is, other than to make contact with 
the original paper, as I think the Gibbs distribution was exactly what Max was trying to 
avoid by taking the zero-temperature limit.}

\section{Conclusion}
\label{sec:conclusion}

%\bibliography{example_paper}
%\bibliographystyle{icml2013}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
