\documentclass{article}
\usepackage{import, subfiles}
\usepackage{hyperref, icml2014}
\usepackage{tabularx}
\input{latex-defs.tex}

\icmltitlerunning{Boosted Mirror Descent}

\begin{document} 

\twocolumn[
\icmltitle{Boosted Mirror Descent, Frank-Wolfe, and Herding}

\icmlauthor{Jonathan Huggins \NA{add footnote on authorship}}{jhuggins@mit.edu}
\icmladdress{MIT CSAIL,
             77 Massachusetts Avenue, Cambridge, MA 02139 USA}
\icmlauthor{Jacob Steinhardt}{jsteinhardt@cs.stanford.edu}
\icmladdress{Stanford University,
             353 Serra Street, Stanford, CA 94305 USA}


% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boosting, optimization, mirror descent, frank-wolfe, conjugate gradient descent, herding}

\vskip 0.3in
]

\begin{abstract} 
We introduce a unifying framework for understanding herding and 
related algorithms. The framework is based on pair of optimization 
algorithms we call primal and dual boosted mirror descent.
We define two classes of generalized herding algorithms, 
including one for infinite dimensional spaces
that may apply even when herding does not. A number 
of convergence bounds that apply to herding and generalized
herding are obtained. Along the way we provide
a more cohesive picture of the connections between herding, 
conditional gradient descent and related methods, and maximum entropy. 
\end{abstract} 

\subfile{introduction.tex}
\subfile{boosted-mirror-descent.tex}
\subfile{herding-connection.tex}
\subfile{convergence-proofs.tex}
\subfile{lower-bounds.tex}
%\subfile{chen-proofs.tex}
\subfile{conclusion.tex}

\bibliography{herding}
\bibliographystyle{icml2014}

\end{document} 
