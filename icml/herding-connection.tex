\documentclass[paper.tex]{subfiles}
\begin{document}
\section{Application to Herding}
\label{sec:herding}

The standard setup for herding is as follows: we are given a 
space $\sX$ and a feature mapping $\phi : \sX \to \sH$ for some 
reproducing kernel Hilbert space $\sH$ \cite{RKHS}. 
We define $\Delta_{\sX}$ to be the simplex 
of probability distributions over $\sX$ and the marginal polytope 
$\sM \eqdef \{\bE_{\mu}[\phi(x)] \mid \mu \in \Delta_{\sX}\}$.

The goal of herding is to construct samples $x_1,\ldots,x_T \in \sX$ 
such that $\|\frac{1}{T}\sum_{t=1}^T \phi(x_t)-\bar{\phi}\|_{\sH}^2$ 
is minimized for some given $\bar{\phi} \in \sM$.

The herding algorithm uses the following updates\footnote{The original 
algorithm includes an additional $\bar{\phi}$ term in the third bullet 
point and indexes $x_t$ as $x_{t+1}$.} [\cite{chen}, equations (1) and (2)]:
\begin{itemize}
\item $w_1 = \bar{\phi}$
\item $x_t \in \arg\max_{x \in \sX} \ip{w_t}{\phi(x)}$
\item $w_{t+1} = \sum_{s=1}^t [\bar{\phi}-\phi(x_s)]$
\end{itemize}
If we define $\theta_t = -w_t/t$, then these updates are equivalent to
\begin{itemize}
\item $\theta_1 = -\bar{\phi}$
\item $\mu_t \in \arg\min_{\mu \in \Delta_{\sX}} \ip{\theta_t}{\bE_{\mu}[\phi(x)]}$
\item $\theta_{t+1} = \frac{1}{t} \sum_{s=1}^t [\bE_{\mu_s}[\phi(x)] - \bar{\phi}]$
\end{itemize}
Note that we have replaced the point $x_t$ with a distribution $\mu_t$. 
This does not change the algorithm as $\bE_{\mu}[\theta_t^T\phi(x)]$ will 
always be minimized at an extreme point of $\Delta_{\sX}$, which corresponds 
to a point mass.

The above updates correspond to Algorithm 2 for $U = \sX$, $\Theta = \sH$, 
$\alpha_t = 1$, $h \equiv 0$, 
$R(\theta) = \ip{\bar{\phi}}{\theta} + \frac{1}{2}\|\theta\|_{\sH}^2$, 
and $\imath(u)$ the canonical extension of $\phi$ from $\sX$ to $\Delta_{\sX}$.
To see this, note that 
\begin{align}
\lefteqn{\arg\min_{\theta \in \sH} \ip{\imath(u)}{\theta} - R(\theta)} \\
 &= \arg\min_{\theta \in \sH} \ip{\bE_{u}[\phi(x)]-\bar{\phi}}{\theta} - \frac{1}{2}\|\theta\|_{\sH}^2 \\
 &= \bE_u[\phi(x)] - \bar{\phi}.
\end{align}
For $u = 0$, this gives us the first bullet point. For $u = \hat{u}_t$, 
this gives us the third bullet point. Finally, the second bullet point 
is already in the form given in Algorithm 2.

Note that $R$ is convex with respect to $\|\cdot\|_2$ and that 
$R^*(u) = \frac{1}{2}\|\bar{\phi}-\bE_{u}[\phi(x)]\|_2^2$. We can 
therefore apply Corollary~\ref{thm:method-2} to obtain the bound
\begin{align*}
\lefteqn{\frac{1}{2}\|\bar{\phi}-\frac{1}{T}\sum_{t=1}^T \phi(x_t)\|_2^2} \\
 &\leq \inf_{u^*} \frac{1}{2}\|\bar{\phi}-\bE_{u^*}[\phi(x)]\|_2^2 + \frac{2r^2\log(T+1)}{T} \\
 &= \frac{2r^2\log(T+1)}{T},
\end{align*}
where $r \eqdef \sup_{m \in \sM} \|m\|_2$.

This is a slightly weaker version of the typical herding bound on MMD.
Note that if we use $\alpha_t = t$, which corresponds to using a weighted 
average, then we obtain a stronger bound of $\frac{8r^2}{T}$ instead.

\subsection{Herding in Infinite Dimensions}
\label{sec:infinite-case}

The convergence bound for herding implicitly assumes that 
$\sM$ is bounded in the $l^2$ norm. Assuming that $\sH$ is 
finite-dimensional, this will always be true. However, in 
the infinite-dimensional case this may no longer be true. 
For instance, consider $\sX = \{1,\ldots,N\}$ and let 
$\phi : \sX \to l^2(\bZ_{\geq 0})$ be
defined by $\phi(n)_i = \frac{1}{\sqrt{n+i}}$. Then 
$\|\phi(n)\|_2 = \infty$ for each $n \in \sX$, and so herding 
yields no convergence guarantees. Indeed, the algorithm 
does not even necessarily make sense: since $\theta$ is 
no longer guaranteed to have finite $l^2$ norm, the 
inner product $\ip{\imath(u)}{\theta}$ may not be defined.

Note, however, that in this case
$\|\phi(n)\|_3 = \sqrt[3]{\sum_{i=0}^{\infty} (n+i)^{-3/2}} < \infty$.
(In fact, the maximum is equal to
$\|\phi(1)\|_3 = \sqrt[3]{\zeta(3/2)} < 1.378$.) This motivates 
a generalization of herding in which we minimize 
$\|\bE_{u}[\phi(x)]-\bar{\phi}\|_3^3$ instead of 
$\|\bE_{u}[\phi(x)]-\bar{\phi}\|_2^2$.

More generally, for any 
$p \geq 2$ and $q$ such that $\frac{1}{p} + \frac{1}{q} = 1$, we can take 
$h \equiv 0$, $R(\theta) = \ip{\bar{\phi}}{\theta} + \frac{1}{q}\|\theta\|_q^q$. 
Then $R^*(u) = \frac{1}{p}\|\bE_{u}[\phi(x)]-\bar{\phi}\|_p^p$. Define the 
``correction function'' $C_q$ by
\begin{equation}
C_q(x)_i \eqdef \sign(x_i) \cdot |x_i|^{\frac{1}{q-1}}.
\end{equation}
We obtain the following \emph{generalized herding} updates:
\begin{itemize}
\item $\theta_{1} = C_q(-\bar{\phi})$
\item $x_{t} \in \arg\min_{x \in \sX} \ip{\phi(x)}{\theta_t}$
\item $\theta_{t+1} = C_q\left(\frac{1}{t} \sum_{s=1}^t [\phi(x_s)-\bar{\phi}]\right)$
\end{itemize}
By Proposition~\ref{prop:method-2}, we know that
\begin{align}
\frac{1}{p}\|\bar{\phi}-\frac{1}{T}\sum_{t=1}^T \phi(x_t)\|_p^p \leq \frac{1}{A_T} \sum_{t=1}^T A_tD_{R^*}(\hat{u}_{t+1}\|\hat{u}_t).
\end{align}
We can no longer use Corollary~\ref{cor:method-2} since $\|\cdot\|_p^p$ is not strongly convex. 
Instead, we use the following two inequalities (proved in Appendix~\ref{apx:inequalities}):
\begin{lemma}
Let $x, y \in \bR_{\geq 0}$. Then:
\begin{equation}
x^p-py^{p-1}x+(p-1)y^p \leq \binom{p}{2}(x-y)^2\max(x,y)^{p-2}
\end{equation}
and
\begin{equation}
(y-z)^2\max(x,y)^{p-2} \leq 4\max(x,y,z)^p
\end{equation}
\end{lemma}
Using these, we obtain the bound
\begin{align}
D_{R*}(\hat{u}_{t+1} \| \hat{u}_t) &= \frac{1}{p} \sum_{i} \hat{u}_{t+1,i}^p - p\hat{u}_{t+1,i}\hat{u}_{t,i}^{p-1} + (p-1)\hat{u}_{t,i}^{p} \\
 &\leq \frac{1}{p}\binom{p}{2} \sum_i (\hat{u}_{t+1,i}-\hat{u}_{t,i})^2\max(\hat{u}_{t+1,i},\hat{u}_{t,i})^{p-2} \\
 &= \frac{1}{p}\binom{p}{2}\frac{\alpha_{t+1}^2}{A_{t+1}^2} \sum_i (u_{t+1,i}-\hat{u}_{t})^2\max(\hat{u}_{t+1,i},\hat{u}_{t,i})^{p-2} \\
 &\leq 2(p-1)\frac{\alpha_{t+1}^2}{A_{t+1}^2} \sum_i \max(\hat{u}_{t,i},\hat{u}_{t+1,i},u_{t+1,i})^p \\
 &\leq 2(p-1)\frac{\alpha_{t+1}^2}{A_{t+1}^2} \sum_i \hat{u}_{t,i}^p+\hat{u}_{t+1,i}^p+u_{t+1,i}^p \\
 &\leq 6(p-1)\frac{\alpha_{t+1}^2}{A_{t+1}^2} r_p^p
\end{align}
Plugging back in yields a convergence rate of $\frac{6(p-1)r_p^p(\log(T)+1)}{T}$ for $\alpha_t = 1$ 
and $\frac{24(p-1)r_p^p}{T}$ for $\alpha_t = t$. We therefore obtain a convergence rate of $O(T^{-1/p})$ 
in the $l^p$ norm, even if $\sM$ is not bounded in the $l^2$ norm.

\subsection{Herding and Maximum Entropy}
\label{sec:max-ent}

Bach et al.~have observed that herding appears to be approximately building 
samples from the maximum entropy distribution for a given collection of moment 
constraints. In this section we present a herding-style algorithm that more 
explicitly yields a maximum-entropy distribution. Our hope is that comparing 
the two algorithms may help to make the connection between herding and maximum 
entropy more explicit.

The key idea is to use the primal regularizer $h(u) = -\eta H(u)$, where 
$H(u)$ is the entropy of $u$ and $\eta \in \bR_{>0}$. We will still use 
$R(\theta) = \ip{\bar{\phi}}{\theta} + \frac{1}{2} \|\theta\|_{\sH}^2$.
In this case, Algorithm 2 gives the updates
\begin{itemize}
\item $\theta_1 = -\bar{\phi}$
\item $u_t(x) \propto \exp\left(-\frac{1}{\eta}\ip{\theta_t}{\phi(x)}\right)$
\item $\theta_{t+1} = \bE_{\hat{u}_t}[\phi(x)] - \bar\phi$.
\end{itemize}
Corollary~\ref{cor:method-2} implies that, for 
$\hat{u}_T = \frac{1}{T} \sum_{t=1}^T u_t$, we have
\begin{align*}
\lefteqn{-\eta H(\hat{u}_T) + \|\bar{\phi}-\bE_{\hat{u}_T}[\phi(x)]\|_{\sH}^2} \\
 &\leq \inf_{u} -\eta H(u) + \|\bar{\phi}-\bE_{u}[\phi(x)]\|_{\sH}^2 + \frac{2r^2\log(T+1)}{T} \\
 &\leq -\eta H_{\max}(\bar{\phi}) + \frac{2r^2\log(T+1)}{T},
\end{align*}
where $H_{\max}(\bar{\phi})$ is the maximum entropy of any distribution whose 
moments are $\bar{\phi}$.

\end{document}
