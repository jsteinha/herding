\documentclass[paper.tex]{subfiles}
\begin{document}

\section{Introduction} 
\label{sec:intro}

The herding algorithm of \citet{Welling:2009a} has been significantly analyzed and extended since its introduction. Herding was originally conceived of as a deterministic method for performing joint learning and inference in Markov random field (MRFs). The ``learning'' interpretation of herding was derived as a fixed step size gradient descent algorithm on the zero temperature limit of a the maximum entropy optimization problem for the MRF. Herding for MRFs was further explored in \citet{Welling:2009a,Gelfand:2010,Bornn:2013}.

In order to define herding, let $\sX$ be an observation space, $\phi : \sX \to \sB_{\Theta}$ a feature map to an inner product space, and $\bar\phi = \bE_{x \sim p}[\phi(x)]$ the empirical moments of some (possible empirical) distribution. Then the herding algorithm is given by the follow simple procedure to generate pseudosamples from $\sX$:
\[
x_{t} &\in \argmin_{x \in \sX} \ip{\theta_{t}}{\phi(x)} \\
\theta_{t} &= \theta_{t-1}  + \bar \phi - \phi(x_{t}).
\]

An alternative interpretation of herding \citep{Chen:2010a,Huszar:2012,Bach:2012a} is as a moment matching algorithm. In particular, it generates a set of pseudosamples $\{ x_{t} \}$  which match the moments $\bar\phi$. That is, the squared error $\sE_{T}^{2} \eqdef \| \bar\phi - T^{-1}\sum_{t=1}^{T} \phi(x_{t})\|^{2} \to 0$ as $T \to \infty$.  If $\sB_{\Theta}$ is a reproducing kernel Hilbert space (RKHS) with feature map $\phi$, $\sE_{T}^{2}$ is equivalent to the maximum mean discrepancy (MMD) between $p$ and the empirical distribution $\hat p_{T} \eqdef T^{-1}\sum_{t=1}^{T} \delta_{x_{t}}$ \citep{Huszar:2012}:
\(
\sE_{T}^{2} &= \MMD^{2}(p, \hat p_{T})  \\
&\eqdef \sup_{\theta \in \Theta} \left|\bE_{p}\ip{\theta}{\phi(x)} - \bE_{\hat p_{T}}\ip{\theta}{\phi(x)} \right|,
\)
where $\Theta = \{ \theta \in \sB_{\Theta} \ | \  \|\theta\| = 1 \}$.

If we are interested in generating samples from $p$ (or some distribution with the same moments as $p$), herding provides an attractive alternative to standard i.i.d. sampling for two reasons. First, it is deterministic, so we do not have to worry about getting ``unlucky'' and generating a set of samples that poorly represent  $p$. Second, in the case where $\sB_{\theta}$ is finite dimensional, the squared error of the herding estimator converges to zero at a rate of $O(1/T^{2})$ whereas the convergence rate of i.i.d.~sampling is $O(1/T)$ \citep{Chen:2010a}. The infinite dimensional case, however, is not as well understood. \citet{Bach:2012a} recently showed that the $O(1/T^{2})$ convergence proof of \citet{Chen:2010a} does not apply in the infinite dimensional case. In addition, \citet{Bach:2012a} introduced an interpretation of herding as a special case of conditional gradient descent (\cgd) minimizing the square norm of an RKHS. They showed that \cgd with line search leads to faster convergence rates: $O(1/T)$ in the infinite dimensional case and exponential in the finite dimensional case. However, they found  empirically that line search had poor approximation properties since led to distributions far from the maximum entropy distribution. 

In this paper, we use a novel gradient descent algorithm to investigate herding and its generalizations:
\begin{itemize}
\item In Section \ref{sec:algorithm} we introduce a novel pair of convex optimization algorithms, which we call primal and dual boosted mirror descent (\primal and \dual). We show how \primal and \dual relate to \cgd and mirror descent (\md). 
\item We then use \dual in Section \ref{sec:herding} to define a class of generalized herding algorithms. Connections with the ``learning'' interpretation are made by showing how to obtain herding as the limit of an entropically normalized case of \dual. 
\item Section \ref{sec:proofs} presents convergence rates for \primal and \dual. As special cases, we recover a $O(\log T/ T)$ convergence rate for herding and a $O(1/T)$ rate for a herding-like algorithm that uses unequally weighted samples. Both of these results apply to the infinite dimensional case. 
\item In Section \ref{sec:lower-bounds} we prove matching lower bounds to the upper bounds given in Section \ref{sec:proofs}, showing that any ``sparse'' herding-like method cannot converge faster than $O(1/T)$ in the the infinite dimensional case and that herding converges at the slower $O(\log T / T)$ rate.
\item Finally, in section \ref{sec:chen} we generalize the $O(1/T^{2})$ convergence proof of \citet{Chen:2010a} to a class of herding-like algorithms derived from \bmd. 
\end{itemize}


\end{document}
