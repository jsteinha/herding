\documentclass[paper.tex]{subfiles}
\begin{document}

\section{Conclusion} 
\label{sec:conclusion}

In this paper we have introduced the primal and dual boosted mirror descent algorithms in order to investigate the properties of herding and a number of novel classes of generalized herding algorithms. $\alpha_{t}$-weighted herding is very similar to standard herding, but the pseudosamples are not required to have equal weights. We also defined $q$-herding. The $q$-herding algorithm minimizes the distance between the sample estimator and the target moments as measured by the $\ell^{p}$ norm (with $p$ the conjugate of $q$, $p \ge 2$). For $p = 2$, we get herding. But in infinite dimensional space, $q$-herding with $p > 2$ may some times be applied when standard herding is inapplicable. 

Using \bmd we also place the convergence results of \citet{Chen:2010} and \citet{Bach:2010} into a single framework. Furthermore, we were able to extend those convergence results to certain classes of herding-like algorithms including $\alpha_{t}$-weighted herding, $q$-herding, and \NA{chen} herding.

Our results thus extend the applicability and flexibility of deterministic algorithms for generating pseudosamples. While not universally applicable, these deterministic algorithms offer an increasingly attractive alternative to random sampling for certain types of problems and model classes because the can offer faster convergence rates, convergence guarantees without ``with high probability'' conditions, and simple implementations. 

\end{document}
