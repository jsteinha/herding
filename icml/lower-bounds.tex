\documentclass[paper.tex]{subfiles}

\begin{document}
\section{Lower Bounds on Convergence Rates}
\label{sec:lower-bounds}

It has been an open question whether it is possible to achieve the fast $O(1/T^{2})$ convergence of finite-dimensional herding in the infinite-dimensional case. We now show that this is not in general possible. In particular, the upper bounds in the previous section for herding and herding-like algorithms have matching lower bounds for at least some combinations of distributions and feature functions. 

Let $\sX$ be the set of vectors $(x_{1},x_{2},\dots) \in [0,1]^{\infty}$ such that $\sum_{n=1}^{\infty} x_{n} = 1$ and a finite number of the $x_{n}$ are non-zero. Define the infinite feature vector $\phi$ to be the imbedding of $\sX$ into $\ell^{1}$. Let the target distribution $p(x)$ be chosen such that $\bar\phi_{n} = c_{\alpha}/\alpha(n)$, where $\alpha(n)$ is any superlinear function (that is, $\alpha \in \omega(n)$) and $c_{\alpha}^{-1} = \sum_{n=1}^{\infty} \alpha(n) < \infty$. We will consider the standard $\ell^{2}$ herding loss function $L(\theta) = \|\theta - \bar \phi\|_2^2$. 

Define a {\em sparse update method} to be any method that produces an estimator of $\bar \phi$ at iteration $t$ of the form 
\[
\theta_{t} = \sum_{s =1}^{t}w_{t} \phi(x_{t}).
\]
In other words, the $t$-th estimator will have at most $t$ non-zero entries, and so 
\[
\|x-x^{*}\|_2^2 \ge c_{\alpha}^{2}\sum_{n > t} 1/\alpha(n)^{2} = \Theta(1/\beta(t)) \label{eq:convergence-lb}
\]
for some $\beta \in \omega(t)$. But for any $\beta \in \omega(t)$ we can choose some $\alpha \in \omega(n)$ to obtain \eqref{eq:convergence-lb}. Therefore, any sparse update method must have convergence rate no faster than $\Theta(1/\beta(t))$ for all superlinear $\beta$, and hence no faster than $\Theta(1/t)$. 

Sparse update methods can be viewed as charactering ``herding-like algorithms,'' including the set of generalized herding algorithms based on \dual and \cgd with line search. Those sparse methods are of interest because they provide (weighted) ``samples'' matching the moments of the distribution of interest. Since they only operate on the extreme points of the simplex, the optimization problems required to use them are more tractable. 

\NA{JH: move definition and discussion of sparse update method to beginning}

\end{document}
