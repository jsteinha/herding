\documentclass[paper.tex]{subfiles}

\begin{document}
\section{A General Lower Bound on Convergence Rates}
\label{sec:lower-bounds}

Although it is possible to achieve the fast $O(1/T^{2})$ convergence rate for finite-dimensional herding, \citet{Bach:2012a} note that without further assumptions herding has only a $O(1/T)$ convergence rate in the infinite-dimensional case. We now provide an example showing this bound is tight for {\em any} herding-like algorithm.  By ``herding-like,'' we mean algorithms that approximate the distribution with at most one additional sample at each step. We formalize this idea by defining a {\em sparse update method} to be any method that produces an estimator of $\bar \phi$ at iteration $t$ of the form 
\[
\theta_{t} = \sum_{s =1}^{t}w_{t} \phi(x_{t}),
\]
where $w_{t} \in \bR$. Sparse update methods include herding, $\alpha_{t}$-weighted herding, \cgd, and  \cgd with line search. These sparse methods -- which only operate on the extreme points of the simplex -- are of interest because they provide (weighted) ``samples'' matching the moments of the distribution of interest. 

Our example is constructed as follows. Let the sample space be $\sX = \{ 1, 2, 3, \dots\}$. Define the infinite feature vector $\phi$ to be $\phi_{n}(x) = \bI(x = n)$, so $\sM$ is the unit $\ell^{1}$ ball. For target distribution $p \in \Delta_{\sX}$, we therefore have  that the mean vector is the vector of probabilities: $\bar\phi_{n} = p(n)$. Let $p(x) = c/\alpha(n)$, where $\alpha(n)$ is any superlinear function (i.e.~$\alpha \in \omega(n)$) and $c^{-1} = \sum_{n=1}^{\infty} \alpha(n) < \infty$. Without loss of generality we can assume that $\alpha$ is monotonically increasing.  We will consider the standard $\ell^{2}$ herding loss function $L(\theta) = \|\theta - \bar \phi\|_2^2$. 

Since we are only considering sparse update methods, the $t$-th estimator will have at most $t$ non-zero entries, and so 
\[
\|\theta_{t}- \bar \phi\|_2^2 \ge c^{2}\sum_{n > t} 1/\alpha(n)^{2} = \Theta(1/\beta(t)) \label{eq:convergence-lb}
\]
for some (monotonically increasing) superlinear function $\beta$. But for any such $\beta$ we can choose some $\alpha$ to obtain \eqref{eq:convergence-lb}. In particular, let $1/\gamma(t) = \frac{\dee}{\dee t}\frac{1}{\beta(t)}$, where $\gamma(t) \in \omega(t^{2})$. Then we can choose $\alpha = \sqrt \gamma \in \omega(t)$ to satisfy \eqref{eq:convergence-lb}. Therefore, any sparse update method must have convergence rate no faster than $\Theta(1/\beta(t))$ for all superlinear $\beta$, and hence no faster than $\Theta(1/t)$. 



\end{document}
