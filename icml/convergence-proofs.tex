\documentclass[paper.tex]{subfiles}

\begin{document}
\section{Convergence Proofs}
\label{sec:proofs}
We now prove the convergence results cited in Section~\ref{sec:algorithm}. 
Throughout this section, assume that $\alpha_1,\ldots,\alpha_T$ is 
a sequence of real numbers and that $A_t = \sum_{s=1}^t \alpha_s$. 
We further require that $A_t \geq 0$ for all $t$.

Also recall that the Bregman divergence is defined by 
$D_f(x_2 \| x_1) \eqdef f(x_2) - \langle \nabla f(x_1), x_2-x_1 \rangle - f(x_1)$.

Our proofs hinge on the following key lemma:
%%% MAIN LEMMA %%%
\begin{lemma}
\label{lem:bregman}
Let $z_1,\ldots,z_T$ be vectors and let $f(x)$ be a strictly convex 
function. Define $\hat{z}_t$ to be $\frac{1}{A_t} \sum_{s=1}^t \alpha_s z_s$.

Let $x_1,\ldots,x_T$ be chosen via $x_{t+1} = \arg\min_{x} f(x) + \langle \hat{z}_t, x\rangle$. 
Then for any $x^*$ we have
\begin{align*}
\lefteqn{\frac{1}{A_T} \sum_{t=1}^T \{\alpha_t(f(x_t) + \langle z_t, x_t \rangle)\}} \\
\phantom{+} &\leq f(x^*) + \langle \hat{z}_t, x^* \rangle + \frac{1}{A_T} \sum_{t=1}^T A_t D_{f}(x_t \| x_{t+1}). 
\end{align*}
\end{lemma}
\begin{proof}
First note that, if $x_0 = \arg\min f(x) + \langle z, x \rangle$, 
then $\nabla f(x_0) = -z$.

%\NA{JNS: In the following proof, (2) is by the update formula for $x_{t+1}$ together with the preceding observation.  (3) is applying (2). (4) and (5) are re-arranging the sum. (6) is definition of Bregman divergence. (7) is the observation again. (8) is the definition of $x_{T+1}$.}

Now note that
\begin{align}
\alpha_{t}z_{t} 
&= A_{t}\hat z_{t} - A_{t-1}\hat z_{t-1} \\
&= - A_{t}\nabla f(x_{t+1}) + A_{t-1} \nabla f(x_{t}),
\end{align}
so we have
\begin{align}
\lefteqn{\sum_{t=1}^T \{\alpha_t(f(x_t) + \langle z_t, x_t \rangle)\}} \\
 &= \sum_{t=1}^T \{\alpha_t f(x_t) + \langle A_{t-1} \nabla f(x_t) - A_t \nabla f(x_{t+1}), x_t \rangle\} \\
 &= \sum_{t=1}^T \{\alpha_t f(x_t) - \langle A_{t} \nabla f(x_{t+1}), x_t-x_{t+1} \rangle\} \\
 & \quad - A_{T}\langle \nabla f(x_{T+1}), x_{T+1} \rangle
\end{align}
\begin{align}
 &= \sum_{t=1}^T \{A_t f(x_t) - \langle A_{t} \nabla f(x_{t+1}), x_t-x_{t+1} \rangle - A_t f(x_{t+1})\}  \\
 &\quad+ A_T(f(x_{T+1}) - \langle \nabla f(x_{T+1}), x_{T+1} \rangle) \nonumber \\
 &= \sum_{t=1}^T \{A_tD_f(x_t \| x_{t+1})\}  \\
 &\quad+ A_T(f(x_{T+1}) - \langle \nabla f(x_{T+1}), x_{T+1} \rangle) \nonumber \\
 &= \sum_{t=1}^T \{A_tD_f(x_t \| x_{t+1})\} + A_T(f(x_{T+1}) + \langle \hat{z}_T, x_{T+1} \rangle) \\
 &\leq \sum_{t=1}^T \{A_tD_f(x_t \| x_{t+1})\} + A_T(f(x^*) + \langle \hat{z}_T, x^* \rangle). 
\end{align}
Dividing both sides by $A_T$ completes the proof.
\end{proof}
We also note that $D_f(x_t \| x_{t+1}) = D_{f^*}(\hat{z}_{t+1} \| z_t)$, where $f^*(z) = \sup_x \langle z,x\rangle - f(x)$. 
This form of the bound will often be more useful to us.
\begin{lemma}
\label{lem:convexity}
Suppose that $D_f(x' \| x) \geq \frac{1}{2}\|x-x'\|^2$ for some 
norm $\|\cdot\|$. (In this case we say that $f$ is strongly 
convex with respect to $\|\cdot\|$.) 
Then $D_{f^*}(x' \| x) \leq \frac{1}{2}\|x-x'\|_{*}^2$.
\end{lemma}

%%% ALGORITHM 1 ANALYSIS %%%
% Convergence proposition
\begin{proposition}[Convergence of Algorithm 1]
\label{prop:method-1}
Consider the updates $\theta_t \in \arg\max_{\theta} \langle \theta, u_t \rangle - R(\theta)$ 
and $u_{t+1} \in \arg\min_u h(u) + \langle \hat{\theta}_s, u \rangle$. 
Then we have
\begin{equation}
\sup_{\theta} L(\hat{u}_T, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_h(x_t \| x_{t+1}).
\end{equation}
\end{proposition}
\begin{proof}
Note that $L(u_t, \theta_t) = \max_{\theta} L(u_t, \theta)$ by construction. 
Also note that, if we invoke Lemma~\ref{lem:bregman} with $f = h$ and 
$z_t = \theta_t$ then we get the inequality
\begin{align}
\frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) 
&\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u^*, \theta_t) \\
&\quad + \frac{1}{A_T} \sum_{t=1}^T A_tD_h(x_t \| x_{t+1}). \nonumber
\end{align}
Combining these together, we get the string of inequalities
\begin{align*}
L(\hat{u}_T, \theta) &= L\left(\frac{1}{A_T} \sum_{t=1}^T \alpha_tu_t, \theta\right) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u^*, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_t D_h(u_t \| u_{t+1}) \\
 &\leq \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_h(u_t \| u_{t+1}),
\end{align*}
as was to be shown.
\end{proof}

% Convergence rate theorem
\begin{theorem}
\label{thm:method-1}
Suppose that $h$ is strongly convex with respect to a norm $\|\cdot\|$ 
and let $r = \sup_{\theta} \|\theta\|_{*}$. Then 
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2}. \]
\end{theorem}
\begin{proof}
By Lemma~\ref{lem:convexity}, we have 
\begin{align*}
D_h(u_t \| u_{t+1}) &= D_{h^*}(\hat{\theta}_{t+1} \| \hat{\theta}_{t}) \\
 &\leq \frac{1}{2}\|\hat\theta_{t+1}-\hat\theta_{t}\|_{*}^2 \\
 &= \frac{1}{2}\|\frac{\sum_{s \leq t}\alpha_s\theta_s}{\sum_{s \leq t} \alpha_s} - \frac{\sum_{s \leq t+1}\alpha_s\theta_s}{\sum_{s \leq t+1} \alpha_s}\|_{*}^2 \\
 &= \frac{1}{2}\|\frac{\alpha_{t+1}}{A_tA_{t+1}} \sum_{s \leq t} \alpha_s\theta_s - \frac{\alpha_{t+1}}{A_{t+1}} \theta_{t+1}\|_{*}^2 \\
 &\leq \frac{1}{2} \left(\frac{\alpha_{t+1}}{A_tA_{t+1}} \sum_{s \leq t} \alpha_s\|\theta_s\|_{*}^2 + \frac{\alpha_{t+1}}{A_{t+1}} \|\theta_{t+1}\|_{*}^2\right)^2 \\
 &\leq \frac{2r^2\alpha_{t+1}^2}{A_{t+1}^2}.
\end{align*}
It follows that 
\begin{align*}
\frac{1}{A_T} \sum_{t=1}^T A_tD_h(u_t \| u_{t+1}) &\leq \frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2}.
\end{align*}
\end{proof}

% Convergence rate corollary 
\begin{corollary} 
\label{cor:method-1}
Under the hypotheses of Theorem \ref{thm:method-1}, for $\alpha_{t} = 1$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2 (\log (T) + 1)}{T}. \]
and for $\alpha_t = t$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{8r^2}{T}. \]
\end{corollary}
\begin{proof}
If we let $\alpha_t = 1$, then $A_t = t$ and $\frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} = \frac{t}{(t+1)^{2}} \le \frac{1}{t}$.
We therefore get
\begin{equation}
\frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} \leq \frac{2r^2}{T} \sum_{t=1}^{T}\frac{1}{t} \leq \frac{2r^2(\log (T) + 1)}{T+1}.
\end{equation}
If we let $\alpha_t = t$, then $A_t = \frac{t(t+1)}{2}$ and 
$\frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} = \frac{2(t+1)^2t(t+1)}{(t+1)^2(t+2)^2} = \frac{2t(t+1)}{(t+2)^2} \leq 2$.
We therefore get
\begin{equation}
\frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2} \leq \frac{4r^2}{T(T+1)} \sum_{t=1}^{T}2 = \frac{8r^2}{T+1},
\end{equation}
which completes the proof.
\end{proof}

%%% ALGORITHM 2 ANALYSIS %%%
% Convergence proposition
\begin{proposition}[Convergence of Algorithm 2]
\label{prop:method-2}
Consider the updates $u_t \in \arg\min_{u} h(u) + \langle \theta_t, u \rangle$ 
and $\theta_{t+1} \in \arg\max_{\theta} \langle \theta, \hat{u}_t \rangle - R(\theta)$. 
Then we have 
\begin{equation}
\sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_{R}(\theta_t \| \theta_{t+1}).
\end{equation}
\end{proposition}
\begin{proof}
If we invoke Lemma~\ref{lem:bregman} with $f=R$ and $z_t=-u_t$, then we get 
the inequality
\begin{align}
\frac{1}{A_T} \sum_{t=1}^T -\alpha_t L(u_t, \theta_t) 
&\leq \frac{1}{A_T} \sum_{t=1}^T -\alpha_t L(u_t, \theta^*) \\
&\quad - \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}). \nonumber
\end{align}
Re-arranging yields
\begin{align}
\frac{1}{A_T }\sum_{t=1}^T \alpha_t L(u_t, \theta^*) 
&\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) \\
&\quad+ \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}). 
\end{align}
Now, we have the following string of inequalities:
\begin{align*}
L(\hat{u}, \theta) &= L\left(\frac{1}{A_T} \sum_{t=1}^T \alpha_t u_t, \theta\right) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u_t, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_t D_R(\theta_t \| \theta_{t+1}) \\
 &= \frac{1}{A_T} \sum_{t=1}^T \alpha_t \inf_{u} L(u, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t L(u^*, \theta_t) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}) \\
 &\leq \frac{1}{A_T} \sum_{t=1}^T \alpha_t \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}) \\
 &= \sup_{\theta} L(u^*, \theta) + \frac{1}{A_T} \sum_{t=1}^T A_tD_R(\theta_t \| \theta_{t+1}),
\end{align*}
as was to be shown.
\end{proof}

% Convergence rate theorem
\begin{theorem}
\label{thm:method-2}
Suppose that $R$ is strongly convex with respect to a norm $\|\cdot\|$ 
and let $r = \sup_{u} \|u\|_{*}$. Then 
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2}{A_T} \sum_{t=1}^T \frac{\alpha_{t+1}^2A_t}{A_{t+1}^2}. \]
\end{theorem}
\begin{corollary}
\label{cor:method-2}
Under the hypotheses of Theorem \ref{thm:method-2}, for $\alpha_t = 1$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{2r^2(\log(T) + 1)}{T} \]
and for $\alpha_t = t$ we have
\[ \sup_{\theta} L(\hat{u}, \theta) \leq \sup_{\theta} L(u^*, \theta) + \frac{8r^2}{T} \]
\end{corollary}
\begin{proof}[Proofs]
The proofs are identical to those for Algorithm 1.
\end{proof}

\end{document}
