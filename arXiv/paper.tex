\documentclass[reqno,oneside,a4paper]{amsart}
\usepackage{import, subfiles}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{chngpage}
\input{latex-defs.tex}

\allowdisplaybreaks

\begin{document} 

\title{Conjugate Gradient, Boosting, and Herding: \\ 
	   Connections and Application}
	   
\thanks{The authors contributed equally to this work.}

\author[J.~H.~Huggins]{Jonathan H.~Huggins}
\address{Massachusetts Institute of Technology}
\urladdr{http://jhuggins.org/}
\email{jhuggins@mit.edu}

\author[J.~Steinhardt]{Jacob Steinhardt}
\address{Stanford University}
\urladdr{http://cs.stanford.edu/~jsteinhardt/}
\email{jsteinhardt@cs.stanford.edu}

\keywords{boosting, optimization, mirror descent, Frank-Wolfe, conjugate gradient descent, herding, graph sparsification}

\begin{abstract} 
Much has been learned about herding since it was first proposed.
We introduce a unifying framework for understanding herding and 
related algorithms. The framework is based on a pair of optimization 
algorithms that we call primal and dual boosted mirror descent.
We define two classes of generalized herding algorithms, 
including one for infinite dimensional spaces
that may apply even when herding does not. A number 
of convergence bounds that apply to herding and generalized
herding are obtained. Along the way we provide
a more cohesive picture of the connections between herding, 
conditional gradient descent, and maximum entropy. 
\end{abstract} 

\maketitle

\subfile{introduction.tex}
\subfile{boosted-mirror-descent.tex}
\subfile{herding-connection.tex}
\subfile{convergence-proofs.tex}
\subfile{lower-bounds.tex}
\subfile{chen-proofs.tex}
\subfile{conclusion.tex}

\bibliography{herding}
\bibliographystyle{icml2014}

\end{document} \maketitle
