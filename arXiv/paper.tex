\documentclass[reqno,oneside,a4paper]{amsart}
\usepackage{import, subfiles}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{chngpage}
%\usepackage{fullpage}
\input{latex-defs.tex}

\allowdisplaybreaks

\begin{document} 

\title{Boosting, Sparsity, and Conditional Gradient: A Tutorial}
%Conjugate Gradient, Boosting, and Herding: \\ 
%	   Connections and Application}
	   
\thanks{Both authors contributed equally to this work.}

\author[J.~H.~Huggins]{Jonathan H.~Huggins}
\address{Massachusetts Institute of Technology}
\urladdr{http://jhuggins.org/}
\email{jhuggins@mit.edu}

\author[J.~Steinhardt]{Jacob Steinhardt}
\address{Stanford University}
\urladdr{http://cs.stanford.edu/~jsteinhardt/}
\email{jsteinhardt@cs.stanford.edu}

\keywords{boosting, optimization, mirror descent, Frank-Wolfe, conditional gradient descent, herding, graph sparsification}

\begin{abstract} 
Many recent papers in machine learning and theoretical computer science have 
focused on the idea of sparse or low-rank ``approximations'' to some object: 
for instance, graph sparsification \citep{deCarliSilva:2011}, low-rank SDP approximation \citep{Arora:2007}, 
and pseudosampling \citep{Welling:2009a,Chen:2010a}. In this paper we show that, when presented with 
a strongly convex optimization problem whose dual problem is linear, we can obtain 
sparse approximate solutions, and use this to re-derive (and in some cases 
extend) many of the known sparsification results. Our main technical tool
is a generalization of the Frank-Wolfe (a.k.a.\ conditional gradient 
descent) algorithm. 
% Much has been learned about herding since it was first proposed.
% We introduce a unifying framework for understanding herding and 
% related algorithms. The framework is based on a pair of optimization 
% algorithms that we call primal and dual boosted mirror descent.
% We define two classes of generalized herding algorithms, 
% including one for infinite dimensional spaces
% that may apply even when herding does not. A number 
% of convergence bounds that apply to herding and generalized
% herding are obtained. Along the way we provide
% a more cohesive picture of the connections between herding, 
% conditional gradient descent, and maximum entropy. 
\end{abstract} 

\maketitle

\subfile{introduction.tex}
\subfile{sparsity-result.tex}
%\subfile{boosted-mirror-descent.tex}
\subfile{herding-connection.tex}
\subfile{convergence-proofs.tex}
\subfile{lower-bounds.tex}
\subfile{chen-proofs.tex}
\subfile{conclusion.tex}

\bibliography{herding}
\bibliographystyle{plainnat} %icml2014}

\end{document} \maketitle
