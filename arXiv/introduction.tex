\documentclass[paper.tex]{subfiles}
\begin{document}

\section{Introduction} 
\label{sec:intro}

The purpose of this paper is to serve as a tutorial on sparsification as it arises 
in machine learning, computer science theory, and optimization. While most of the 
results presented are known, they are often distributed across the these 
literatures. Our objective is to bring them all into one place, 
as well as to provide a unifying perspective.

By a ``sparsification result'', we mean, loosely, a method for approximating an 
object (such as a matrix or a probability distribution) by a sparse linear 
combination of a pre-defined set of atoms. For instance, \emph{graph 
sparsification} consists of approximating the adjacency matrix of a graph by a 
weighted matrix that contains only a small number of edges \cite{?}. 
\emph{Low-rank matrix approximation} consists of approximating a matrix by a 
linear combination of a small number of rank-$1$ matrices \cite{?}. 
\emph{Pseudo-sampling} (also known as \emph{herding} \cite{?}) consists of 
approximating the moments of a probability distribution using a weighted 
combination of a small number of ``pseudo-samples'', e.g. 
$\bE_{x \sim \pi}[\phi(x)] \approx \sum_{i=1}^k w_i \phi(x_i)$.

Other sparsification results include:  approximating the 
Nash equilibrium of a game by a weighted mixture of a small number of pure 
strategies \cite{?}; finding low-complexity circuits that are indistinguishable 
from a high-complexity circuit relative to a given oracle \cite{?}; and 
approximating the maximum flow by a small number of resistive flows \cite{?}.

In this paper, we will define a generalized notion of sparsity that captures 
all of the above examples, and provide general sparsification algorithms that 
can then be applied to each of these settings as special cases. However, in some 
cases these general results are not enough to yield the best known bounds; we will 
state when this is the case and cite the best bounds of which we are currently 
aware.

In writing this tutorial we are indebted to several authors who have already 
published unifying results between different notions of sparsity. 

\section{A Generalized Sparsity Result}
Throughout, we will let $S$ denote a 
compact subset of $\bR^n$ and let $\Conv(S)$ denote the convex hull of $S$ 
(which will always be a closed set due to compactness of $S$). Two particularly 
important examples are:
\begin{example}
\label{ex:simplex}
Let $S$ consist of the $n$ standard unit vectors $e_1, \ldots, e_n$. Then 
$\Conv(S)$ is the $n$-dimensional simplex $\Delta_n$.
\end{example}

\begin{example}
\label{ex:sdp}
Let $S = \{vv^{\top} \mid \|v\|_2 = 1\}$. An alternative characterization 
of $S$ is all rank-$1$, positive semidefinite matrices with trace $1$. The 
convex hull of $S$ is $\Conv(S) = \{V \mid V \succeq 0, \Tr(V) = 1\}$.
\end{example}

\begin{definition}
Given a set $S \subseteq \bR^n$, a \emph{$(S, k)$-sparse} point is a point 
$x \in \Conv(S)$ that is a convex combination of at most $k$ extreme 
points of $\Conv(S)$.
\end{definition}

Recall that an extreme point is a point that cannot be written as a convex 
combination of other points in $\Conv(S)$. Importantly, every extreme point 
of $\Conv(S)$ lies in $S$. So, in Example~\ref{ex:simplex}, every 
$(S,k)$-sparse point is also $k$-sparse in the usual sense. In 
Example~\ref{ex:sdp}, every $(S,k)$-sparse point has rank at most $k$, since 
it is in the convex hull of $k$ rank-$1$ matrices.

\begin{example}
\label{ex:marginal-polytope}
Given a space $\sX$ and a function $\phi : \sX \to \bR^d$, let 
$S = \{\phi(x) \mid x \in \sX\}$. Then $\Conv(S)$ is called the 
\emph{marginal polytope} of $\sX$ with respect to $\phi$. The 
marginal polytope consists of all possible expected values of 
$\phi(x)$ with respect to an arbitrary probability distribution 
on $\sX$. A $(S,k)$-sparse point in $\Conv(S)$ is a point that 
can be written as a weighted average $\sum_{i=1}^k w_i \phi(x_i)$. 
In this case, we call the $x_i$ \emph{pseudosamples}.
\end{example}

\begin{example}
\label{ex:graph}
Given an undirected graph $G$ with vertex set $\{1,\ldots,n\}$, let $E$ 
denote its edge set, and let $m = |E|$. For a given 
edge $e = (i,j)$, define $x_e \in \bR^n$ to be the vector with 
$1$ in the $i$th coordinate, $-1$ in the $j$th coordinate, and 
$0$s everywhere else. Let $X_e \eqdef mx_ex_e^{\top}$. 
Define $S$ to be $\{X_e \mid e \in E\}$.

The \emph{Laplacian} $L$ of $G$ is defined to be the matrix $L$ 
with $L_{ij} = -1$ if $(i,j)$ is an edge, and $L_{ii}$ equal to 
the degree of vertex $i$. It can be written as $L = \sum_{e \in E} x_ex_e^{\top}$. 
In particular, $L \in \Conv(S)$.

A $(S,k)$-sparse point can be written as a convex combination 
\[ \sum_{r=1}^k w_rX_{e_r} = \sum_{r=1}^k (m\cdot w_r)x_{e_r}x_{e_r}^{\top} \]
for some $k$ edges $e_1,\ldots,e_k$. Any such matrix is the Laplacian 
of a \emph{weighted} graph (with edge weight $m\cdot w_r$ on edge $e_r$) 
with $k$ edges and total edge weight $m$.
\end{example}

With these examples in mind, we are now ready to state our main result:
\begin{theorem}
\label{thm:main}
Given vector spaces $V$ and $W$, let $f : V \times W \to \bR$ be a function 
such that 
\begin{enumerate}
\item $f(\cdot,w)$ is linear for each fixed $w$
\item $f(v, \cdot)$ is strongly convex with respect to a norm $\|\cdot\|$ 
      for each fixed $v$.
\end{enumerate}
Then, for any compact set $S \subset V$, and any point $x \in \Conv(S)$, 
there exists a $(S,k)$-sparse point $y$ such that 
\[ \max_{w \in W} f(y,w) \leq \max_{w \in W} f(x,w) + \frac{2D^2}{k+1}, \]
where $D = \sup_{s, s' \in S} \|s-s'\|_*$ and $\|\cdot\|_*$ is the dual norm 
of $\|\cdot\|$.
\end{theorem}
The proof of Theorem~\ref{thm:main} is given in Section~\ref{todo}. For now, 
to give a flavor for the relevance of Theorem~\ref{thm:main}, we show how 
to obtain a sparsification result for Example~\ref{ex:marginal-polytope}.
\begin{proposition}
\label{prop:pseudosamples}
Let $\phi : \sX \to \bR^d$ and let $\sM = \Conv(\{\phi(x) \mid x \in \sX\})$ 
be the marginal polytope as defined in Example~\ref{ex:marginal-polytope}. 
Let $D$ be the diameter of $\sM$ in the $l^2$ norm. Then, for any 
$\mu \in \sM$, there exists points $x_1,\ldots,x_k$ and weights 
$w_1,\ldots,w_k$ such that
\[ \left\|\mu - \sum_{i=1}^k w_i \phi(x_i)\right\|_2 \leq \sqrt{\frac{4D^2}{k+1}} \]
\end{proposition}
\begin{proof}
Let $V = W = \bR^d$, and apply Theorem~\ref{thm:main} to the function 
$f(v,w) = (v-\mu)^{\top}w - \frac{1}{2}\|w\|_2^2$. The function $f$ is 
linear in $v$ for each $w$, and is strongly convex in $w$ with respect to the 
$l^2$ norm for each $v$. Also note that 
$\sup_{w \in W} f(v,w) = \frac{1}{2}\|v-\mu\|_2^2$. Since the dual of the 
$l^2$ norm is the $l^2$ norm, Theorem~\ref{thm:main} says that there exists 
a $(S,k)$-sparse point $y$ such that
\[ \frac{1}{2}\|\mu - y\|_2^2 \leq \frac{1}{2}\|\mu - \mu\|_2^2 + \frac{2D^2}{k+1}. \]
Since $\|\mu-\mu\|_2 = 0$ and any $(S,k)$-sparse point $y$ can be written 
as $\sum_{i=1}^k w_i \phi(x_i)$, we have the desired result.
\end{proof}
Proposition~\ref{prop:pseudosamples} says that, given any distribution 
$\pi$ over $\sX$, we can find $O(1/\epsilon^2)$ pseudosamples that approximate 
the moments of $\pi$ with accuracy $\epsilon$ in the $l^2$ norm. We note 
that in some instances this can be improved to $O(1/\epsilon)$, see 
for instance \cite{chen}.

We will establish many more such results in the coming sections.

\end{document}
